{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35b3499d-dc9c-405a-8714-a26341b581e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.sparse as sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5acedcb-89e6-41c6-8969-bf3437fc7898",
   "metadata": {},
   "source": [
    "# Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6053118-029e-4d7c-9c02-6f988508e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, embedding_dim, n_layers, user_ids, item_ids, interaction_scores=None):\n",
    "        \"\"\"\n",
    "        LightGCN model\n",
    "        Args:\n",
    "            n_users (int): Number of users\n",
    "            n_items (int): Number of items\n",
    "            embedding_dim (int): Embedding dimension\n",
    "            n_layers (int): Number of propagation layers\n",
    "            user_ids (list or tensor): List of user indices\n",
    "            item_ids (list or tensor): List of item indices\n",
    "            interaction_scores (list or tensor, optional): Interaction scores (binary or weighted)\n",
    "        \"\"\"\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Initialize user and item embeddings using normal initialization\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.1)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.1)\n",
    "\n",
    "        # Build adjacency matrix from user-item interactions\n",
    "        self.adj_matrix = self.build_adj_matrix(user_ids, item_ids, interaction_scores)\n",
    "\n",
    "        # Softplus function for BPR loss\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def build_adj_matrix(self, user_ids, item_ids, interaction_scores):\n",
    "        \"\"\"\n",
    "        Build the sparse adjacency matrix from user-item interactions.\n",
    "        Args:\n",
    "            user_ids (list or tensor): User indices for each interaction\n",
    "            item_ids (list or tensor): Item indices for each interaction\n",
    "            interaction_scores (list or tensor, optional): Interaction scores (optional, binary if None)\n",
    "        Returns:\n",
    "            adj_matrix (torch.sparse.FloatTensor): Symmetrically normalized sparse adjacency matrix\n",
    "        \"\"\"\n",
    "        if interaction_scores is None:\n",
    "            interaction_scores = torch.ones(len(user_ids))  # Default to binary interactions if no scores are provided\n",
    "\n",
    "        # Number of total nodes (users + items)\n",
    "        n_total_nodes = self.n_users + self.n_items\n",
    "\n",
    "        # Prepare the adjacency matrix in coordinate (COO) format\n",
    "        user_tensor = torch.tensor(user_ids, dtype=torch.long)\n",
    "        item_tensor = torch.tensor(item_ids, dtype=torch.long) + self.n_users  # Shift item indices by n_users\n",
    "        score_tensor = torch.tensor(interaction_scores, dtype=torch.float32)\n",
    "\n",
    "        # Create user-item interaction edges (user-to-item and item-to-user)\n",
    "        indices = torch.cat([user_tensor.unsqueeze(0), item_tensor.unsqueeze(0)], dim=0)\n",
    "        values = score_tensor\n",
    "\n",
    "        # Create the sparse user-item adjacency matrix\n",
    "        adj_matrix = torch.sparse_coo_tensor(indices, values, (n_total_nodes, n_total_nodes))\n",
    "\n",
    "        # Symmetric normalization\n",
    "        row_sum = torch.sparse.sum(adj_matrix, dim=1).to_dense()  # Compute row sums (degree for each node)\n",
    "        d_inv_sqrt = torch.pow(row_sum, -0.5)  # Compute D^-0.5\n",
    "        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0  # Handle divide by zero for isolated nodes\n",
    "\n",
    "        # Normalize the adjacency matrix: A_hat = D^-0.5 * A * D^-0.5\n",
    "        d_mat_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "        adj_matrix = torch.sparse.mm(d_mat_inv_sqrt, adj_matrix)  # D^-0.5 * A\n",
    "        adj_matrix = torch.sparse.mm(adj_matrix, d_mat_inv_sqrt)  # (D^-0.5 * A) * D^-0.5\n",
    "\n",
    "        return adj_matrix\n",
    "\n",
    "    def propagate(self):\n",
    "        \"\"\"\n",
    "        Perform embedding propagation based on the LightGCN propagation rule.\n",
    "        \"\"\"\n",
    "        user_embeddings = self.user_embedding.weight\n",
    "        item_embeddings = self.item_embedding.weight\n",
    "\n",
    "        all_embeddings = torch.cat([user_embeddings, item_embeddings], dim=0)  # Stack user and item embeddings\n",
    "\n",
    "        all_layer_embeddings = [all_embeddings]\n",
    "\n",
    "        # Perform K-layer propagation\n",
    "        for _ in range(self.n_layers):\n",
    "            all_embeddings = torch.sparse.mm(self.adj_matrix, all_embeddings)\n",
    "            all_layer_embeddings.append(all_embeddings)\n",
    "\n",
    "        # Combine embeddings from all layers (mean aggregation)\n",
    "        final_embeddings = torch.stack(all_layer_embeddings, dim=1).mean(dim=1)\n",
    "\n",
    "        final_user_embeddings = final_embeddings[:self.n_users]  # First part is users\n",
    "        final_item_embeddings = final_embeddings[self.n_users:]  # Second part is items\n",
    "\n",
    "        return final_user_embeddings, final_item_embeddings\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        \"\"\"\n",
    "        Perform forward pass to get final user and item embeddings.\n",
    "        Args:\n",
    "            users (tensor): User indices\n",
    "            items (tensor): Item indices\n",
    "        \"\"\"\n",
    "        user_embeddings, item_embeddings = self.propagate()\n",
    "\n",
    "        # Get specific user and item embeddings\n",
    "        return user_embeddings[users], item_embeddings[items]\n",
    "\n",
    "    def predict(self, users, items):\n",
    "        \"\"\"\n",
    "        Predict interaction score for given users and items.\n",
    "        \"\"\"\n",
    "        user_emb, item_emb = self.forward(users, items)\n",
    "        return torch.sum(user_emb * item_emb, dim=1)\n",
    "\n",
    "    def bpr_loss(self, users, pos, neg):\n",
    "        \"\"\"\n",
    "        Compute the BPR loss for the model.\n",
    "        Args:\n",
    "            users (tensor): User indices\n",
    "            pos (tensor): Positive item indices\n",
    "            neg (tensor): Negative item indices\n",
    "        Returns:\n",
    "            tuple: BPR loss and regularization loss\n",
    "        \"\"\"\n",
    "        # Get embeddings for users, positive items, and negative items\n",
    "        user_emb, pos_emb = self.forward(users, pos)\n",
    "        _, neg_emb = self.forward(users, neg)\n",
    "\n",
    "        # Compute the positive and negative scores (dot product between embeddings)\n",
    "        pos_scores = torch.sum(user_emb * pos_emb, dim=1)\n",
    "        neg_scores = torch.sum(user_emb * neg_emb, dim=1)\n",
    "\n",
    "        # Compute BPR loss using Softplus\n",
    "        bpr_loss = torch.mean(self.softplus(neg_scores - pos_scores))\n",
    "\n",
    "        # Compute regularization loss (L2 norm of embeddings)\n",
    "        reg_loss = (1 / 2) * (user_emb.norm(2).pow(2) + pos_emb.norm(2).pow(2) + neg_emb.norm(2).pow(2)) / float(len(users))\n",
    "\n",
    "        return bpr_loss, reg_loss\n",
    "\n",
    "# Sample training loop using the interaction table format\n",
    "def train_lightgcn(model, train_data, n_items, epochs=10, lr=0.001, reg=1e-4):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for user, pos_item in train_data:\n",
    "            # Randomly sample a negative item\n",
    "            neg_item = torch.randint(0, n_items, (1,)).item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get embeddings\n",
    "            user_embeddings, pos_item_embeddings = model(torch.tensor([user]), torch.tensor([pos_item]))\n",
    "            _, neg_item_embeddings = model(torch.tensor([user]), torch.tensor([neg_item]))\n",
    "\n",
    "            # Compute BPR loss\n",
    "            loss = bpr_loss(user_embeddings, pos_item_embeddings, neg_item_embeddings, reg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a71da-1ee3-474c-b3f4-4488d2a45dd3",
   "metadata": {},
   "source": [
    "# Test implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce5326d7-ec9d-422c-bbd8-03c837a23e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0006,  0.0022, -0.0067], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Example data (assuming you have 3 users, 4 items, and binary interactions)\n",
    "user_ids = [0, 0, 1, 2, 2]\n",
    "item_ids = [0, 1, 2, 3, 1]\n",
    "n_users = len(set(user_ids))\n",
    "n_items = len(set(item_ids))\n",
    "interaction_scores = [1, 1, 1, 1, 1]  # Binary interaction scores\n",
    "\n",
    "# Instantiate the model\n",
    "model = LightGCN(n_users=3, n_items=4, embedding_dim=64, n_layers=3, user_ids=user_ids, item_ids=item_ids, interaction_scores=interaction_scores)\n",
    "\n",
    "# Example forward pass\n",
    "users = torch.tensor([0, 1, 2])\n",
    "items = torch.tensor([0, 1, 2])\n",
    "predictions = model.predict(users, items)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0b1fea-0376-4624-ad8a-d000208a97f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock User IDs: [3, 4, 2, 4, 4, 1, 2, 2, 2, 4, 3, 2, 4, 1, 3, 1, 3, 4, 0, 3]\n",
      "Mock Item IDs: [9, 5, 8, 0, 9, 2, 6, 3, 8, 2, 4, 2, 6, 4, 8, 6, 1, 3, 8, 1]\n",
      "Interaction Scores: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Epoch 1/5, Loss: 3.4653, Reg Loss: 0.0291\n",
      "Epoch 2/5, Loss: 3.4652, Reg Loss: 0.0278\n",
      "Epoch 3/5, Loss: 3.4669, Reg Loss: 0.0266\n",
      "Epoch 4/5, Loss: 3.4651, Reg Loss: 0.0236\n",
      "Epoch 5/5, Loss: 3.4659, Reg Loss: 0.0231\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Fixing random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Constants\n",
    "n_users = 5\n",
    "n_items = 10\n",
    "n_interactions = 20  # Number of interactions\n",
    "embedding_dim = 8\n",
    "n_layers = 3\n",
    "batch_size = 4\n",
    "\n",
    "# Create mock user-item interactions (randomly generated)\n",
    "user_ids = np.random.randint(0, n_users, size=n_interactions)\n",
    "item_ids = np.random.randint(0, n_items, size=n_interactions)\n",
    "interaction_scores = np.ones(n_interactions)  # All interactions are positive\n",
    "\n",
    "# Display mock dataset\n",
    "print(\"Mock User IDs:\", user_ids.tolist())\n",
    "print(\"Mock Item IDs:\", item_ids.tolist())\n",
    "print(\"Interaction Scores:\", interaction_scores)\n",
    "\n",
    "\n",
    "class InteractionDataset(Dataset):\n",
    "    def __init__(self, user_ids, item_ids, n_items):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_ids (list or array): List of user indices.\n",
    "            item_ids (list or array): List of item indices.\n",
    "            n_items (int): Number of unique items for negative sampling.\n",
    "        \"\"\"\n",
    "        self.user_ids = user_ids\n",
    "        self.item_ids = item_ids\n",
    "        self.n_items = n_items\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user = self.user_ids[idx]\n",
    "        pos_item = self.item_ids[idx]\n",
    "        \n",
    "        # Randomly sample a negative item for the user\n",
    "        neg_item = np.random.randint(0, self.n_items)\n",
    "        while neg_item == pos_item:\n",
    "            neg_item = np.random.randint(0, self.n_items)\n",
    "        \n",
    "        return user, pos_item, neg_item\n",
    "\n",
    "# Create dataset\n",
    "interaction_dataset = InteractionDataset(user_ids, item_ids, n_items)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(interaction_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate LightGCN model\n",
    "model = LightGCN(n_users=n_users, n_items=n_items, embedding_dim=embedding_dim, n_layers=n_layers,\n",
    "                 user_ids=user_ids, item_ids=item_ids, interaction_scores=interaction_scores)\n",
    "\n",
    "# Define optimizer (e.g., Adam)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    total_reg_loss = 0\n",
    "    \n",
    "    for batch_idx, (users, pos_items, neg_items) in enumerate(dataloader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Compute BPR loss and regularization loss\n",
    "        bpr_loss, reg_loss = model.bpr_loss(users, pos_items, neg_items)\n",
    "        \n",
    "        # Total loss = BPR loss + regularization loss\n",
    "        loss = bpr_loss + reg_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += bpr_loss.item()\n",
    "        total_reg_loss += reg_loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Loss: {total_loss:.4f}, Reg Loss: {total_reg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6c998-5fc4-43f7-8e19-0f469bb0374b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
