{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8f6e8d-f776-4d39-898c-d783c5ae3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b3499d-dc9c-405a-8714-a26341b581e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from loguru import logger\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.sparse as sparse\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5acedcb-89e6-41c6-8969-bf3437fc7898",
   "metadata": {},
   "source": [
    "# Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6053118-029e-4d7c-9c02-6f988508e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_utils import mse_loss, train\n",
    "from src.model import LightGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "182f6ed4-bdd3-4703-b987-e67c44c94f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 23:49:53.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mUsing cpu device\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# device = (\n",
    "#     \"cuda\"\n",
    "#     if torch.cuda.is_available()\n",
    "#     else \"mps\"\n",
    "#     if torch.backends.mps.is_available()\n",
    "#     else \"cpu\"\n",
    "# )\n",
    "device = 'cpu'\n",
    "logger.info(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a71da-1ee3-474c-b3f4-4488d2a45dd3",
   "metadata": {},
   "source": [
    "# Test implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce5326d7-ec9d-422c-bbd8-03c837a23e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.3578e-03,  5.6214e-03, -1.4180e-05], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Mock data\n",
    "user_ids = [0, 0, 1, 2, 2]\n",
    "item_ids = [0, 1, 2, 3, 1]\n",
    "interaction_scores = [1, 4, 5, 3, 2]\n",
    "n_users = len(set(user_ids))\n",
    "n_items = len(set(item_ids))\n",
    "\n",
    "val_user_ids = [0, 1, 2]\n",
    "val_item_ids = [2, 1, 2]\n",
    "val_interaction_scores = [2, 4, 5]\n",
    "\n",
    "model = LightGCN(embedding_dim=64, n_layers=3, user_ids=user_ids, item_ids=item_ids, interaction_scores=interaction_scores, device=device)\n",
    "\n",
    "# Example forward pass\n",
    "users = torch.tensor([0, 1, 2])\n",
    "items = torch.tensor([0, 1, 2])\n",
    "predictions = model.predict(users, items)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c0b1fea-0376-4624-ad8a-d000208a97f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock User IDs: [0, 0, 1, 2, 2]\n",
      "Mock Item IDs: [0, 1, 2, 3, 1]\n",
      "Interaction Scores: [1, 4, 5, 3, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvquys/frostmourne/reco-algo/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c805a7ed3e6c4a57b302e310f261ed52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 23:49:54.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1, Gradient Norm for user_embedding.weight: 0.062471\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1, Gradient Norm for item_embedding.weight: 0.065761\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 1, Total Gradient Norm: 0.090704\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 1, Learning Rate: 0.001000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 1, Global Loss: 11.5081\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2, Gradient Norm for user_embedding.weight: 0.074053\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2, Gradient Norm for item_embedding.weight: 0.110462\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 2, Total Gradient Norm: 0.132988\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 2, Learning Rate: 0.001000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 2, Global Loss: 10.2538\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mEpoch 1, Loss: 10.2538\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mEpoch 1, Validation Loss: 14.9949\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 23:49:54.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3, Gradient Norm for user_embedding.weight: 0.068096\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3, Gradient Norm for item_embedding.weight: 0.069900\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 3, Total Gradient Norm: 0.097586\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 3, Learning Rate: 0.001000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 3, Global Loss: 13.5060\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4, Gradient Norm for user_embedding.weight: 0.020492\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4, Gradient Norm for item_embedding.weight: 0.052784\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 4, Total Gradient Norm: 0.056622\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 4, Learning Rate: 0.001000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 4, Global Loss: 7.2551\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mEpoch 2, Loss: 7.2551\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mEpoch 2, Validation Loss: 14.9953\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 23:49:54.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5, Gradient Norm for user_embedding.weight: 0.059056\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5, Gradient Norm for item_embedding.weight: 0.065333\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 5, Total Gradient Norm: 0.088068\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 5, Learning Rate: 0.001000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 5, Global Loss: 7.5072\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6, Gradient Norm for user_embedding.weight: 0.138600\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6, Gradient Norm for item_embedding.weight: 0.110497\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 6, Total Gradient Norm: 0.177256\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 6, Learning Rate: 0.001000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 6, Global Loss: 16.2515\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mEpoch 3, Loss: 16.2515\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mEpoch 3, Validation Loss: 14.9956\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:54.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mEarly stopping at epoch 3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Fixing random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "embedding_dim = 8\n",
    "n_layers = 3\n",
    "batch_size = 4\n",
    "\n",
    "# Display mock dataset\n",
    "print(\"Mock User IDs:\", user_ids)\n",
    "print(\"Mock Item IDs:\", item_ids)\n",
    "print(\"Interaction Scores:\", interaction_scores)\n",
    "\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, user_ids, item_ids, ratings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_ids (list or array): List of user indices.\n",
    "            item_ids (list or array): List of item indices.\n",
    "            ratings (list or array): List of corresponding ratings.\n",
    "        \"\"\"\n",
    "        self.user_ids = user_ids\n",
    "        self.item_ids = item_ids\n",
    "        self.ratings = ratings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user = self.user_ids[idx]\n",
    "        item = self.item_ids[idx]\n",
    "        rating = self.ratings[idx]\n",
    "        return user, item, rating\n",
    "\n",
    "rating_dataset = RatingDataset(user_ids, item_ids, interaction_scores)\n",
    "dataloader = DataLoader(rating_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_rating_dataset = RatingDataset(val_user_ids, val_item_ids, val_interaction_scores)\n",
    "val_dataloader = DataLoader(val_rating_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate LightGCN model\n",
    "model = LightGCN(embedding_dim=embedding_dim, n_layers=n_layers,\n",
    "                 user_ids=user_ids, item_ids=item_ids, interaction_scores=interaction_scores, device=device)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 50\n",
    "\n",
    "train(model, dataloader, val_dataloader, epochs=n_epochs, patience=2, print_steps=1, lr=0.001, device=device, progress_bar_type='tqdm_notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170dec89-a874-4dce-8f94-07d978fcc5b8",
   "metadata": {},
   "source": [
    "# Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83803362-5eaa-40bb-b28b-878316d5db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\"../data/train.parquet\")\n",
    "val_df = pd.read_parquet(\"../data/val.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0d4bfbb-f3bb-40bc-ae22-23e8d8f9d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.id_mapper import IDMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17d6c998-5fc4-43f7-8e19-0f469bb0374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 23:49:54.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mlen(unique_user_ids)=5,223, len(unique_item_ids)=2,653\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_ids = train_df['user_id'].values\n",
    "item_ids = train_df['parent_asin'].values\n",
    "unique_user_ids = list(set(user_ids))\n",
    "unique_item_ids = list(set(item_ids))\n",
    "\n",
    "logger.info(f\"{len(unique_user_ids)=:,.0f}, {len(unique_item_ids)=:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d652b08a-bdeb-485a-9700-39651c225241",
   "metadata": {},
   "outputs": [],
   "source": [
    "idm = IDMapper()\n",
    "idm.fit(unique_user_ids, unique_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8861682-ac7c-410d-b263-17056ceb2466",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indices = [idm.get_user_index(user_id) for user_id in user_ids]\n",
    "item_indices = [idm.get_item_index(item_id) for item_id in item_ids]\n",
    "ratings = train_df['rating'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e73143ed-5992-4f7d-83d1-d2dc4978af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_user_indices = [idm.get_user_index(user_id) for user_id in val_df['user_id']]\n",
    "val_item_indices = [idm.get_item_index(item_id) for item_id in val_df['parent_asin']]\n",
    "val_ratings = val_df['rating'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260fbe7-2f90-44a1-be74-ce5db9b511ee",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "240a04ed-8898-443f-b0f4-7399bfa63810",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_dataset = RatingDataset(user_indices, item_indices, ratings)\n",
    "dataloader = DataLoader(rating_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_rating_dataset = RatingDataset(val_user_indices, val_item_indices, val_ratings)\n",
    "val_dataloader = DataLoader(val_rating_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22dcf157-425f-4d15-9b53-e1ff93887def",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "n_layers = 3\n",
    "batch_size = 256\n",
    "\n",
    "# Instantiate LightGCN model\n",
    "model = LightGCN(embedding_dim=embedding_dim, n_layers=n_layers,\n",
    "                 user_ids=user_indices, item_ids=item_indices, interaction_scores=ratings, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cbb2a8-a578-4ad2-941e-efd01d930336",
   "metadata": {},
   "source": [
    "#### Predict before train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eb1583b-40f7-4157-a383-5891389ac119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34367</th>\n",
       "      <td>AEHW2B54HDLZ3APBEWXHYLZ6SSYQ</td>\n",
       "      <td>B07MYVF61Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1654225907045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id parent_asin  rating      timestamp\n",
       "34367  AEHW2B54HDLZ3APBEWXHYLZ6SSYQ  B07MYVF61Y     4.0  1654225907045"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = 'AEHW2B54HDLZ3APBEWXHYLZ6SSYQ'\n",
    "val_df.loc[lambda df: df['user_id'].eq(user_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c48fbc80-eda8-4dff-a246-95f8f2f75082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0076], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id = 'B07MYVF61Y'\n",
    "user_indice = idm.get_user_index(user_id)\n",
    "item_indice = idm.get_item_index(item_id)\n",
    "\n",
    "model.predict([user_indice], [item_indice])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd217c-c0c8-4e7c-81fe-6eb36f4729b5",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c63eb300-bda8-4c35-93ad-71a4c2e025fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvquys/frostmourne/reco-algo/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106ccaefda454be38c76bb241ed0e778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/4524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 23:49:56.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 100, Gradient Norm for user_embedding.weight: 0.001257\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:56.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 100, Gradient Norm for item_embedding.weight: 0.292150\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:56.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 100, Total Gradient Norm: 0.292153\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:56.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 100, Learning Rate: 0.030000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:56.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 100, Global Loss: 19.1892\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:58.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 200, Gradient Norm for user_embedding.weight: 0.084955\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:58.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 200, Gradient Norm for item_embedding.weight: 0.000005\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:58.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 200, Total Gradient Norm: 0.084955\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:58.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 200, Learning Rate: 0.030000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:49:58.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 200, Global Loss: 19.2188\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:00.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 300, Gradient Norm for user_embedding.weight: 0.635178\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:00.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 300, Gradient Norm for item_embedding.weight: 0.772364\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:00.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 300, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:00.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 300, Learning Rate: 0.030000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:00.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 300, Global Loss: 19.3679\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:02.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 400, Gradient Norm for user_embedding.weight: 0.037743\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:02.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 400, Gradient Norm for item_embedding.weight: 0.351150\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:02.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 400, Total Gradient Norm: 0.353173\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:02.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 400, Learning Rate: 0.030000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:02.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 400, Global Loss: 19.4192\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:05.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 500, Gradient Norm for user_embedding.weight: 0.216669\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:05.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 500, Gradient Norm for item_embedding.weight: 0.889792\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:05.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 500, Total Gradient Norm: 0.915792\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:05.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 500, Learning Rate: 0.030000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:05.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 500, Global Loss: 19.7066\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:06.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 600, Gradient Norm for user_embedding.weight: 0.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:06.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 600, Gradient Norm for item_embedding.weight: 0.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:06.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 600, Total Gradient Norm: 0.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:06.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 600, Learning Rate: 0.030000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:06.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 600, Global Loss: 19.7876\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:08.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 700, Gradient Norm for user_embedding.weight: 0.738621\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:08.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 700, Gradient Norm for item_embedding.weight: 0.361696\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:08.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 700, Total Gradient Norm: 0.822426\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:08.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 700, Learning Rate: 0.030000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:08.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 700, Global Loss: 19.7249\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:11.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 800, Gradient Norm for user_embedding.weight: 0.781366\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:11.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 800, Gradient Norm for item_embedding.weight: 0.624073\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:11.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 800, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:11.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 800, Learning Rate: 0.030000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:11.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 800, Global Loss: 19.7010\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:12.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 900, Gradient Norm for user_embedding.weight: 0.771506\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:12.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 900, Gradient Norm for item_embedding.weight: 0.636221\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:12.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:12.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 900, Learning Rate: 0.030000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:12.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 900, Global Loss: 19.6430\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:15.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1000, Gradient Norm for user_embedding.weight: 0.817379\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:15.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1000, Gradient Norm for item_embedding.weight: 0.576100\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:15.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 1000, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:15.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 1000, Learning Rate: 0.027000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:15.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 1000, Global Loss: 19.5692\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:16.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1100, Gradient Norm for user_embedding.weight: 0.994282\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:16.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1100, Gradient Norm for item_embedding.weight: 0.106780\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:16.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 1100, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:16.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 1100, Learning Rate: 0.027000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:16.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 1100, Global Loss: 19.5423\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:19.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1200, Gradient Norm for user_embedding.weight: 0.714271\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:19.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1200, Gradient Norm for item_embedding.weight: 0.699869\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:19.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 1200, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:19.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 1200, Learning Rate: 0.027000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:19.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 1200, Global Loss: 19.4178\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:21.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1300, Gradient Norm for user_embedding.weight: 0.672203\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:21.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1300, Gradient Norm for item_embedding.weight: 0.740366\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:21.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 1300, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:21.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 1300, Learning Rate: 0.027000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:21.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 1300, Global Loss: 19.3817\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:23.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1400, Gradient Norm for user_embedding.weight: 0.481612\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:23.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1400, Gradient Norm for item_embedding.weight: 0.876384\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:23.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 1400, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:23.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 1400, Learning Rate: 0.027000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:23.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 1400, Global Loss: 19.3089\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:25.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1500, Gradient Norm for user_embedding.weight: 0.944587\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:25.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1500, Gradient Norm for item_embedding.weight: 0.328261\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:25.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 1500, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:25.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 1500, Learning Rate: 0.027000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:25.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 1500, Global Loss: 19.2454\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:27.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1600, Gradient Norm for user_embedding.weight: 0.866121\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:27.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1600, Gradient Norm for item_embedding.weight: 0.499833\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:27.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 1600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:27.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 1600, Learning Rate: 0.027000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:27.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 1600, Global Loss: 19.1768\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:29.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1700, Gradient Norm for user_embedding.weight: 0.801931\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:29.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1700, Gradient Norm for item_embedding.weight: 0.597417\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:29.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 1700, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:29.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 1700, Learning Rate: 0.027000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:29.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 1700, Global Loss: 19.0767\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:31.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1800, Gradient Norm for user_embedding.weight: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:31.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1800, Gradient Norm for item_embedding.weight: 0.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:31.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 1800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:31.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 1800, Learning Rate: 0.027000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:31.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 1800, Global Loss: 18.9950\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:33.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1900, Gradient Norm for user_embedding.weight: 0.882591\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:33.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 1900, Gradient Norm for item_embedding.weight: 0.470141\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:33.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 1900, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:33.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 1900, Learning Rate: 0.027000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:33.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 1900, Global Loss: 18.8566\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:35.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2000, Gradient Norm for user_embedding.weight: 0.820171\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:35.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2000, Gradient Norm for item_embedding.weight: 0.572118\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:35.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 2000, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:35.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 2000, Learning Rate: 0.024300\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:35.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 2000, Global Loss: 18.7512\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:37.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2100, Gradient Norm for user_embedding.weight: 0.634608\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:37.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2100, Gradient Norm for item_embedding.weight: 0.772834\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:37.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 2100, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:37.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 2100, Learning Rate: 0.024300\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:37.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 2100, Global Loss: 18.6596\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:39.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2200, Gradient Norm for user_embedding.weight: 0.887568\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:39.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2200, Gradient Norm for item_embedding.weight: 0.460676\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:39.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 2200, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:39.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 2200, Learning Rate: 0.024300\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:39.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 2200, Global Loss: 18.5808\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:41.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2300, Gradient Norm for user_embedding.weight: 0.841395\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:41.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2300, Gradient Norm for item_embedding.weight: 0.540421\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:41.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 2300, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:41.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 2300, Learning Rate: 0.024300\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:41.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 2300, Global Loss: 18.4582\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:43.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2400, Gradient Norm for user_embedding.weight: 0.812914\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:43.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2400, Gradient Norm for item_embedding.weight: 0.582383\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:43.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 2400, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:43.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 2400, Learning Rate: 0.024300\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:43.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 2400, Global Loss: 18.3458\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:45.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2500, Gradient Norm for user_embedding.weight: 0.874581\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:45.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2500, Gradient Norm for item_embedding.weight: 0.484878\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:45.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 2500, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:45.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 2500, Learning Rate: 0.024300\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:45.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 2500, Global Loss: 18.2228\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:47.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2600, Gradient Norm for user_embedding.weight: 0.990642\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:47.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2600, Gradient Norm for item_embedding.weight: 0.136483\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:47.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 2600, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:47.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 2600, Learning Rate: 0.024300\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:47.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 2600, Global Loss: 18.1093\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:49.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2700, Gradient Norm for user_embedding.weight: 0.890644\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:49.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2700, Gradient Norm for item_embedding.weight: 0.454700\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:49.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 2700, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:49.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 2700, Learning Rate: 0.024300\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:49.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 2700, Global Loss: 17.9918\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:51.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2800, Gradient Norm for user_embedding.weight: 0.934763\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:51.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2800, Gradient Norm for item_embedding.weight: 0.355270\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:51.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 2800, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:51.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 2800, Learning Rate: 0.024300\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:51.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 2800, Global Loss: 17.8732\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:53.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2900, Gradient Norm for user_embedding.weight: 0.740132\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:53.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 2900, Gradient Norm for item_embedding.weight: 0.672461\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:53.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 2900, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:53.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 2900, Learning Rate: 0.024300\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:53.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 2900, Global Loss: 17.7484\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:55.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3000, Gradient Norm for user_embedding.weight: 0.972758\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:55.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3000, Gradient Norm for item_embedding.weight: 0.231821\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:55.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 3000, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:55.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 3000, Learning Rate: 0.021870\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:55.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 3000, Global Loss: 17.6196\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:57.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3100, Gradient Norm for user_embedding.weight: 0.870187\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:57.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3100, Gradient Norm for item_embedding.weight: 0.492721\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:57.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 3100, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:57.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 3100, Learning Rate: 0.021870\u001b[0m\n",
      "\u001b[32m2024-09-07 23:50:57.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 3100, Global Loss: 17.4922\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:00.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3200, Gradient Norm for user_embedding.weight: 0.828092\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:00.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3200, Gradient Norm for item_embedding.weight: 0.560592\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:00.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 3200, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:00.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 3200, Learning Rate: 0.021870\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:00.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 3200, Global Loss: 17.3328\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:02.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3300, Gradient Norm for user_embedding.weight: 0.973131\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:02.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3300, Gradient Norm for item_embedding.weight: 0.230249\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:02.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 3300, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:02.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 3300, Learning Rate: 0.021870\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:02.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 3300, Global Loss: 17.1997\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:04.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3400, Gradient Norm for user_embedding.weight: 0.799324\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:04.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3400, Gradient Norm for item_embedding.weight: 0.600899\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:04.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 3400, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:04.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 3400, Learning Rate: 0.021870\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:04.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 3400, Global Loss: 17.0456\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:06.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3500, Gradient Norm for user_embedding.weight: 0.976361\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:06.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3500, Gradient Norm for item_embedding.weight: 0.216146\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:06.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 3500, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:06.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 3500, Learning Rate: 0.021870\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:06.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 3500, Global Loss: 16.9216\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:08.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3600, Gradient Norm for user_embedding.weight: 0.799649\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:08.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3600, Gradient Norm for item_embedding.weight: 0.600466\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:08.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 3600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:08.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 3600, Learning Rate: 0.021870\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:08.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 3600, Global Loss: 16.7905\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:10.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3700, Gradient Norm for user_embedding.weight: 0.895027\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:10.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3700, Gradient Norm for item_embedding.weight: 0.446012\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:10.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 3700, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:10.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 3700, Learning Rate: 0.021870\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:10.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 3700, Global Loss: 16.6562\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:12.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3800, Gradient Norm for user_embedding.weight: 0.721520\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:12.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3800, Gradient Norm for item_embedding.weight: 0.692393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:12.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 3800, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:12.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 3800, Learning Rate: 0.021870\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:12.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 3800, Global Loss: 16.5166\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:14.535\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3900, Gradient Norm for user_embedding.weight: 0.918687\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:14.535\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 3900, Gradient Norm for item_embedding.weight: 0.394985\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:14.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 3900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:14.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 3900, Learning Rate: 0.021870\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:14.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 3900, Global Loss: 16.3867\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:16.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4000, Gradient Norm for user_embedding.weight: 0.944727\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:16.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4000, Gradient Norm for item_embedding.weight: 0.327858\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:16.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 4000, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:16.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 4000, Learning Rate: 0.019683\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:16.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 4000, Global Loss: 16.2568\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:18.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4100, Gradient Norm for user_embedding.weight: 0.942848\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:18.633\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4100, Gradient Norm for item_embedding.weight: 0.333223\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:18.633\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 4100, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:18.633\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 4100, Learning Rate: 0.019683\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:18.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 4100, Global Loss: 16.1248\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:20.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4200, Gradient Norm for user_embedding.weight: 0.952591\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:20.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4200, Gradient Norm for item_embedding.weight: 0.304252\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:20.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 4200, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:20.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 4200, Learning Rate: 0.019683\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:20.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 4200, Global Loss: 16.0107\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:22.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4300, Gradient Norm for user_embedding.weight: 0.394898\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:22.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4300, Gradient Norm for item_embedding.weight: 0.918725\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:22.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 4300, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:22.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 4300, Learning Rate: 0.019683\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:22.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 4300, Global Loss: 15.8810\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:24.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4400, Gradient Norm for user_embedding.weight: 0.984142\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:24.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4400, Gradient Norm for item_embedding.weight: 0.177379\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:24.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 4400, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:24.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 4400, Learning Rate: 0.019683\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:24.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 4400, Global Loss: 15.7482\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:26.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4500, Gradient Norm for user_embedding.weight: 0.847175\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:26.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4500, Gradient Norm for item_embedding.weight: 0.531313\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:26.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 4500, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:26.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 4500, Learning Rate: 0.019683\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:26.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 4500, Global Loss: 15.6235\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:27.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mEpoch 1, Loss: 15.5965\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:34.688\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mEpoch 1, Validation Loss: 16.7116\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/4524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 23:51:36.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4600, Gradient Norm for user_embedding.weight: 0.467167\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:36.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4600, Gradient Norm for item_embedding.weight: 0.884168\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:36.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 4600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:36.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 4600, Learning Rate: 0.019683\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:36.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 4600, Global Loss: 5.4515\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:38.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4700, Gradient Norm for user_embedding.weight: 0.470769\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:38.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4700, Gradient Norm for item_embedding.weight: 0.882256\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:38.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 4700, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:38.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 4700, Learning Rate: 0.019683\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:38.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 4700, Global Loss: 6.3031\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:40.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4800, Gradient Norm for user_embedding.weight: 0.624533\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:40.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4800, Gradient Norm for item_embedding.weight: 0.780997\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:40.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 4800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:40.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 4800, Learning Rate: 0.019683\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:40.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 4800, Global Loss: 6.3815\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:42.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4900, Gradient Norm for user_embedding.weight: 0.889803\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:42.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 4900, Gradient Norm for item_embedding.weight: 0.456343\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:42.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 4900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:42.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 4900, Learning Rate: 0.019683\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:42.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 4900, Global Loss: 6.4548\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:44.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5000, Gradient Norm for user_embedding.weight: 0.795748\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:44.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5000, Gradient Norm for item_embedding.weight: 0.605627\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:44.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 5000, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:44.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 5000, Learning Rate: 0.017715\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:44.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 5000, Global Loss: 6.4293\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:46.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5100, Gradient Norm for user_embedding.weight: 0.841348\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:46.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5100, Gradient Norm for item_embedding.weight: 0.540493\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:46.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 5100, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:46.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 5100, Learning Rate: 0.017715\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:46.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 5100, Global Loss: 6.3810\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:48.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5200, Gradient Norm for user_embedding.weight: 0.967998\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:48.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5200, Gradient Norm for item_embedding.weight: 0.250955\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:48.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 5200, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:48.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 5200, Learning Rate: 0.017715\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:48.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 5200, Global Loss: 6.3703\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:50.989\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5300, Gradient Norm for user_embedding.weight: 0.590253\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:50.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5300, Gradient Norm for item_embedding.weight: 0.807217\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:50.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 5300, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:50.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 5300, Learning Rate: 0.017715\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:50.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 5300, Global Loss: 6.3532\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:52.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5400, Gradient Norm for user_embedding.weight: 0.895090\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:52.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5400, Gradient Norm for item_embedding.weight: 0.445884\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:52.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 5400, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:52.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 5400, Learning Rate: 0.017715\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:52.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 5400, Global Loss: 6.3956\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:55.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5500, Gradient Norm for user_embedding.weight: 0.987500\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:55.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5500, Gradient Norm for item_embedding.weight: 0.157614\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:55.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 5500, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:55.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 5500, Learning Rate: 0.017715\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:55.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 5500, Global Loss: 6.3646\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:57.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5600, Gradient Norm for user_embedding.weight: 0.990874\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:57.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5600, Gradient Norm for item_embedding.weight: 0.134787\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:57.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 5600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:57.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 5600, Learning Rate: 0.017715\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:57.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 5600, Global Loss: 6.3973\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:59.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5700, Gradient Norm for user_embedding.weight: 0.766734\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:59.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5700, Gradient Norm for item_embedding.weight: 0.641964\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:59.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 5700, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:59.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 5700, Learning Rate: 0.017715\u001b[0m\n",
      "\u001b[32m2024-09-07 23:51:59.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 5700, Global Loss: 6.4373\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:01.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5800, Gradient Norm for user_embedding.weight: 0.673492\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:01.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5800, Gradient Norm for item_embedding.weight: 0.739194\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:01.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 5800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:01.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 5800, Learning Rate: 0.017715\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:01.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 5800, Global Loss: 6.4278\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:03.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5900, Gradient Norm for user_embedding.weight: 0.465673\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:03.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 5900, Gradient Norm for item_embedding.weight: 0.884956\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:03.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 5900, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:03.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 5900, Learning Rate: 0.017715\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:03.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 5900, Global Loss: 6.4608\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:05.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6000, Gradient Norm for user_embedding.weight: 0.881563\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:05.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6000, Gradient Norm for item_embedding.weight: 0.472066\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:05.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 6000, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:05.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 6000, Learning Rate: 0.015943\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:05.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 6000, Global Loss: 6.4669\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:07.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6100, Gradient Norm for user_embedding.weight: 0.775094\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:07.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6100, Gradient Norm for item_embedding.weight: 0.631845\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:07.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 6100, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:07.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 6100, Learning Rate: 0.015943\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:07.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 6100, Global Loss: 6.4898\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:09.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6200, Gradient Norm for user_embedding.weight: 0.685976\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:09.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6200, Gradient Norm for item_embedding.weight: 0.727624\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:09.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 6200, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:09.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 6200, Learning Rate: 0.015943\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:09.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 6200, Global Loss: 6.4966\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:11.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6300, Gradient Norm for user_embedding.weight: 0.872251\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:11.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6300, Gradient Norm for item_embedding.weight: 0.489057\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:11.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 6300, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:11.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 6300, Learning Rate: 0.015943\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:11.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 6300, Global Loss: 6.4810\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:14.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6400, Gradient Norm for user_embedding.weight: 0.858622\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:14.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6400, Gradient Norm for item_embedding.weight: 0.512609\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:14.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 6400, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:14.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 6400, Learning Rate: 0.015943\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:14.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 6400, Global Loss: 6.4879\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:16.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6500, Gradient Norm for user_embedding.weight: 0.920560\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:16.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6500, Gradient Norm for item_embedding.weight: 0.390599\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:16.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 6500, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:16.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 6500, Learning Rate: 0.015943\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:16.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 6500, Global Loss: 6.5042\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:18.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6600, Gradient Norm for user_embedding.weight: 0.871520\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:18.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6600, Gradient Norm for item_embedding.weight: 0.490359\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:18.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 6600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:18.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 6600, Learning Rate: 0.015943\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:18.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 6600, Global Loss: 6.5307\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:20.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6700, Gradient Norm for user_embedding.weight: 0.897501\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:20.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6700, Gradient Norm for item_embedding.weight: 0.441012\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:20.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 6700, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:20.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 6700, Learning Rate: 0.015943\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:20.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 6700, Global Loss: 6.5282\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:22.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6800, Gradient Norm for user_embedding.weight: 0.930687\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:22.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6800, Gradient Norm for item_embedding.weight: 0.365816\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:22.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 6800, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:22.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 6800, Learning Rate: 0.015943\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:22.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 6800, Global Loss: 6.5500\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:24.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6900, Gradient Norm for user_embedding.weight: 0.859634\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:24.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 6900, Gradient Norm for item_embedding.weight: 0.510909\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:24.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 6900, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:24.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 6900, Learning Rate: 0.015943\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:24.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 6900, Global Loss: 6.5656\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:26.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7000, Gradient Norm for user_embedding.weight: 0.560430\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:26.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7000, Gradient Norm for item_embedding.weight: 0.483959\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:26.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 7000, Total Gradient Norm: 0.740472\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:26.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 7000, Learning Rate: 0.014349\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:26.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 7000, Global Loss: 6.5559\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:28.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7100, Gradient Norm for user_embedding.weight: 0.965283\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:28.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7100, Gradient Norm for item_embedding.weight: 0.261202\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:28.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 7100, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:28.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 7100, Learning Rate: 0.014349\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:28.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 7100, Global Loss: 6.5388\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:30.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7200, Gradient Norm for user_embedding.weight: 0.861798\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:30.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7200, Gradient Norm for item_embedding.weight: 0.507251\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:30.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 7200, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:30.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 7200, Learning Rate: 0.014349\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:30.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 7200, Global Loss: 6.5557\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:32.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7300, Gradient Norm for user_embedding.weight: 0.902475\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:32.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7300, Gradient Norm for item_embedding.weight: 0.430742\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:32.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 7300, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:32.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 7300, Learning Rate: 0.014349\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:32.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 7300, Global Loss: 6.5772\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:35.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7400, Gradient Norm for user_embedding.weight: 0.945021\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:35.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7400, Gradient Norm for item_embedding.weight: 0.327007\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:35.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 7400, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:35.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 7400, Learning Rate: 0.014349\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:35.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 7400, Global Loss: 6.5493\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:37.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7500, Gradient Norm for user_embedding.weight: 0.838091\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:37.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7500, Gradient Norm for item_embedding.weight: 0.545530\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:37.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 7500, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:37.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 7500, Learning Rate: 0.014349\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:37.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 7500, Global Loss: 6.5488\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:39.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7600, Gradient Norm for user_embedding.weight: 0.791573\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:39.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7600, Gradient Norm for item_embedding.weight: 0.611073\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:39.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 7600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:39.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 7600, Learning Rate: 0.014349\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:39.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 7600, Global Loss: 6.5453\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:41.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7700, Gradient Norm for user_embedding.weight: 0.886401\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:41.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7700, Gradient Norm for item_embedding.weight: 0.462918\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:41.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 7700, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:41.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 7700, Learning Rate: 0.014349\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:41.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 7700, Global Loss: 6.5608\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:43.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7800, Gradient Norm for user_embedding.weight: 0.888414\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:43.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7800, Gradient Norm for item_embedding.weight: 0.459041\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:43.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 7800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:43.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 7800, Learning Rate: 0.014349\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:43.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 7800, Global Loss: 6.5605\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:45.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7900, Gradient Norm for user_embedding.weight: 0.961920\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:45.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 7900, Gradient Norm for item_embedding.weight: 0.273330\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:45.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 7900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:45.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 7900, Learning Rate: 0.014349\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:45.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 7900, Global Loss: 6.5393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:47.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8000, Gradient Norm for user_embedding.weight: 0.933674\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:47.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8000, Gradient Norm for item_embedding.weight: 0.358121\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:47.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 8000, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:47.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 8000, Learning Rate: 0.012914\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:47.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 8000, Global Loss: 6.5649\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:49.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8100, Gradient Norm for user_embedding.weight: 0.762742\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:49.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8100, Gradient Norm for item_embedding.weight: 0.646702\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:49.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 8100, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:49.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 8100, Learning Rate: 0.012914\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:49.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 8100, Global Loss: 6.5696\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:51.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8200, Gradient Norm for user_embedding.weight: 0.660965\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:51.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8200, Gradient Norm for item_embedding.weight: 0.750416\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:51.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 8200, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:51.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 8200, Learning Rate: 0.012914\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:51.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 8200, Global Loss: 6.5644\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:53.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8300, Gradient Norm for user_embedding.weight: 0.535656\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:53.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8300, Gradient Norm for item_embedding.weight: 0.844436\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:53.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 8300, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:53.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 8300, Learning Rate: 0.012914\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:53.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 8300, Global Loss: 6.5559\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:55.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8400, Gradient Norm for user_embedding.weight: 0.620692\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:55.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8400, Gradient Norm for item_embedding.weight: 0.784054\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:55.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 8400, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:55.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 8400, Learning Rate: 0.012914\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:55.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 8400, Global Loss: 6.5437\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:57.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8500, Gradient Norm for user_embedding.weight: 0.434256\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:57.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8500, Gradient Norm for item_embedding.weight: 0.270140\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:57.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 8500, Total Gradient Norm: 0.511423\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:57.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 8500, Learning Rate: 0.012914\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:57.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 8500, Global Loss: 6.5490\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:59.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8600, Gradient Norm for user_embedding.weight: 0.647740\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:59.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8600, Gradient Norm for item_embedding.weight: 0.627290\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:59.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 8600, Total Gradient Norm: 0.901698\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:59.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 8600, Learning Rate: 0.012914\u001b[0m\n",
      "\u001b[32m2024-09-07 23:52:59.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 8600, Global Loss: 6.5501\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:01.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8700, Gradient Norm for user_embedding.weight: 0.886862\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:01.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8700, Gradient Norm for item_embedding.weight: 0.462032\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:01.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 8700, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:01.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 8700, Learning Rate: 0.012914\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:01.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 8700, Global Loss: 6.5433\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:03.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8800, Gradient Norm for user_embedding.weight: 0.949706\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:03.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8800, Gradient Norm for item_embedding.weight: 0.313141\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:03.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 8800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:03.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 8800, Learning Rate: 0.012914\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:03.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 8800, Global Loss: 6.5463\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:05.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8900, Gradient Norm for user_embedding.weight: 0.881857\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:05.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 8900, Gradient Norm for item_embedding.weight: 0.471515\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:05.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 8900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:05.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 8900, Learning Rate: 0.012914\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:05.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 8900, Global Loss: 6.5475\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:07.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9000, Gradient Norm for user_embedding.weight: 0.965492\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:07.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9000, Gradient Norm for item_embedding.weight: 0.260433\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:07.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 9000, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:07.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 9000, Learning Rate: 0.011623\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:07.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 9000, Global Loss: 6.5598\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:08.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mEpoch 2, Loss: 6.5568\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:16.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mEpoch 2, Validation Loss: 13.5290\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/4524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 23:53:17.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9100, Gradient Norm for user_embedding.weight: 0.776196\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:17.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9100, Gradient Norm for item_embedding.weight: 0.630490\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:17.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 9100, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:17.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 9100, Learning Rate: 0.011623\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:17.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 9100, Global Loss: 3.4109\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:19.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9200, Gradient Norm for user_embedding.weight: 0.608335\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:19.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9200, Gradient Norm for item_embedding.weight: 0.504105\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:19.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 9200, Total Gradient Norm: 0.790059\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:19.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 9200, Learning Rate: 0.011623\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:19.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 9200, Global Loss: 3.2376\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:21.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9300, Gradient Norm for user_embedding.weight: 0.924220\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:21.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9300, Gradient Norm for item_embedding.weight: 0.381859\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:21.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 9300, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:21.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 9300, Learning Rate: 0.011623\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:21.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 9300, Global Loss: 3.1555\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:24.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9400, Gradient Norm for user_embedding.weight: 0.820000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:24.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9400, Gradient Norm for item_embedding.weight: 0.572362\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:24.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 9400, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:24.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 9400, Learning Rate: 0.011623\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:24.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 9400, Global Loss: 3.1566\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:26.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9500, Gradient Norm for user_embedding.weight: 0.846367\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:26.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9500, Gradient Norm for item_embedding.weight: 0.532598\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:26.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 9500, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:26.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 9500, Learning Rate: 0.011623\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:26.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 9500, Global Loss: 3.1725\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:28.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9600, Gradient Norm for user_embedding.weight: 0.880521\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:28.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9600, Gradient Norm for item_embedding.weight: 0.474005\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:28.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 9600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:28.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 9600, Learning Rate: 0.011623\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:28.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 9600, Global Loss: 3.2832\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:30.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9700, Gradient Norm for user_embedding.weight: 0.823006\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:30.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9700, Gradient Norm for item_embedding.weight: 0.488494\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:30.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 9700, Total Gradient Norm: 0.957060\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:30.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 9700, Learning Rate: 0.011623\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:30.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 9700, Global Loss: 3.3059\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:32.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9800, Gradient Norm for user_embedding.weight: 0.486278\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:32.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9800, Gradient Norm for item_embedding.weight: 0.665690\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:32.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 9800, Total Gradient Norm: 0.824385\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:32.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 9800, Learning Rate: 0.011623\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:32.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 9800, Global Loss: 3.3349\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:35.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9900, Gradient Norm for user_embedding.weight: 0.793851\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:35.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 9900, Gradient Norm for item_embedding.weight: 0.608111\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:35.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 9900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:35.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 9900, Learning Rate: 0.011623\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:35.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 9900, Global Loss: 3.3261\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:37.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10000, Gradient Norm for user_embedding.weight: 0.888659\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:37.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10000, Gradient Norm for item_embedding.weight: 0.458567\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:37.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 10000, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:37.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 10000, Learning Rate: 0.010460\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:37.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 10000, Global Loss: 3.4143\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:39.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10100, Gradient Norm for user_embedding.weight: 0.944874\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:39.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10100, Gradient Norm for item_embedding.weight: 0.327433\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:39.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 10100, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:39.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 10100, Learning Rate: 0.010460\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:39.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 10100, Global Loss: 3.4548\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:41.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10200, Gradient Norm for user_embedding.weight: 0.894859\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:41.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10200, Gradient Norm for item_embedding.weight: 0.446348\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:41.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 10200, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:41.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 10200, Learning Rate: 0.010460\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:41.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 10200, Global Loss: 3.4949\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:43.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10300, Gradient Norm for user_embedding.weight: 0.368826\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:43.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10300, Gradient Norm for item_embedding.weight: 0.304181\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:43.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 10300, Total Gradient Norm: 0.478078\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:43.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 10300, Learning Rate: 0.010460\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:43.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 10300, Global Loss: 3.5270\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:45.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10400, Gradient Norm for user_embedding.weight: 0.801336\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:45.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10400, Gradient Norm for item_embedding.weight: 0.598214\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:45.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 10400, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:45.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 10400, Learning Rate: 0.010460\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:45.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 10400, Global Loss: 3.5618\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:48.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10500, Gradient Norm for user_embedding.weight: 0.649862\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:48.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10500, Gradient Norm for item_embedding.weight: 0.427433\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:48.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 10500, Total Gradient Norm: 0.777830\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:48.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 10500, Learning Rate: 0.010460\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:48.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 10500, Global Loss: 3.5860\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:50.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10600, Gradient Norm for user_embedding.weight: 0.456726\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:50.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10600, Gradient Norm for item_embedding.weight: 0.574076\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:50.289\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 10600, Total Gradient Norm: 0.733595\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:50.289\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 10600, Learning Rate: 0.010460\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:50.289\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 10600, Global Loss: 3.6145\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:52.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10700, Gradient Norm for user_embedding.weight: 0.875838\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:52.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10700, Gradient Norm for item_embedding.weight: 0.482604\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:52.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 10700, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:52.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 10700, Learning Rate: 0.010460\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:52.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 10700, Global Loss: 3.6528\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:54.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10800, Gradient Norm for user_embedding.weight: 0.959493\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:54.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10800, Gradient Norm for item_embedding.weight: 0.281730\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:54.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 10800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:54.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 10800, Learning Rate: 0.010460\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:54.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 10800, Global Loss: 3.6837\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:56.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10900, Gradient Norm for user_embedding.weight: 0.668157\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:56.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 10900, Gradient Norm for item_embedding.weight: 0.744020\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:56.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 10900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:56.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 10900, Learning Rate: 0.010460\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:56.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 10900, Global Loss: 3.7141\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:58.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11000, Gradient Norm for user_embedding.weight: 0.857654\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:58.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11000, Gradient Norm for item_embedding.weight: 0.514226\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:58.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 11000, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:58.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 11000, Learning Rate: 0.009414\u001b[0m\n",
      "\u001b[32m2024-09-07 23:53:58.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 11000, Global Loss: 3.7335\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:01.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11100, Gradient Norm for user_embedding.weight: 0.919845\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:01.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11100, Gradient Norm for item_embedding.weight: 0.392281\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:01.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 11100, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:01.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 11100, Learning Rate: 0.009414\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:01.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 11100, Global Loss: 3.7823\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:03.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11200, Gradient Norm for user_embedding.weight: 0.782082\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:03.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11200, Gradient Norm for item_embedding.weight: 0.623174\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:03.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 11200, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:03.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 11200, Learning Rate: 0.009414\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:03.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 11200, Global Loss: 3.7731\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:05.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11300, Gradient Norm for user_embedding.weight: 0.821940\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:05.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11300, Gradient Norm for item_embedding.weight: 0.569573\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:05.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 11300, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:05.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 11300, Learning Rate: 0.009414\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:05.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 11300, Global Loss: 3.7982\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:07.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11400, Gradient Norm for user_embedding.weight: 0.923517\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:07.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11400, Gradient Norm for item_embedding.weight: 0.383555\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:07.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 11400, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:07.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 11400, Learning Rate: 0.009414\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:07.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 11400, Global Loss: 3.8236\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:09.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11500, Gradient Norm for user_embedding.weight: 0.903795\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:09.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11500, Gradient Norm for item_embedding.weight: 0.427963\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:09.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 11500, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:09.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 11500, Learning Rate: 0.009414\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:09.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 11500, Global Loss: 3.8776\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:11.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11600, Gradient Norm for user_embedding.weight: 0.918104\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:11.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11600, Gradient Norm for item_embedding.weight: 0.396339\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:11.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 11600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:11.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 11600, Learning Rate: 0.009414\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:11.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 11600, Global Loss: 3.8939\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:13.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11700, Gradient Norm for user_embedding.weight: 0.936892\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:13.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11700, Gradient Norm for item_embedding.weight: 0.349617\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:13.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 11700, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:13.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 11700, Learning Rate: 0.009414\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:13.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 11700, Global Loss: 3.9260\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:16.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11800, Gradient Norm for user_embedding.weight: 0.966635\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:16.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11800, Gradient Norm for item_embedding.weight: 0.256154\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:16.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 11800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:16.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 11800, Learning Rate: 0.009414\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:16.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 11800, Global Loss: 3.9384\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:18.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11900, Gradient Norm for user_embedding.weight: 0.426183\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:18.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 11900, Gradient Norm for item_embedding.weight: 0.904637\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:18.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 11900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:18.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 11900, Learning Rate: 0.009414\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:18.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 11900, Global Loss: 3.9720\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:20.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12000, Gradient Norm for user_embedding.weight: 0.459333\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:20.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12000, Gradient Norm for item_embedding.weight: 0.888263\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:20.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 12000, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:20.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 12000, Learning Rate: 0.008473\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:20.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 12000, Global Loss: 3.9756\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:22.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12100, Gradient Norm for user_embedding.weight: 0.877519\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:22.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12100, Gradient Norm for item_embedding.weight: 0.479541\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:22.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 12100, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:22.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 12100, Learning Rate: 0.008473\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:22.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 12100, Global Loss: 3.9821\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:24.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12200, Gradient Norm for user_embedding.weight: 0.974665\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:24.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12200, Gradient Norm for item_embedding.weight: 0.223666\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:24.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 12200, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:24.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 12200, Learning Rate: 0.008473\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:24.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 12200, Global Loss: 3.9956\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:26.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12300, Gradient Norm for user_embedding.weight: 0.933994\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:26.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12300, Gradient Norm for item_embedding.weight: 0.357286\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:26.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 12300, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:26.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 12300, Learning Rate: 0.008473\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:26.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 12300, Global Loss: 4.0086\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:29.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12400, Gradient Norm for user_embedding.weight: 0.324654\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:29.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12400, Gradient Norm for item_embedding.weight: 0.647790\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:29.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 12400, Total Gradient Norm: 0.724591\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:29.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 12400, Learning Rate: 0.008473\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:29.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 12400, Global Loss: 4.0182\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:31.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12500, Gradient Norm for user_embedding.weight: 0.873802\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:31.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12500, Gradient Norm for item_embedding.weight: 0.486279\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:31.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 12500, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:31.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 12500, Learning Rate: 0.008473\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:31.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 12500, Global Loss: 4.0387\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:33.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12600, Gradient Norm for user_embedding.weight: 0.750930\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:33.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12600, Gradient Norm for item_embedding.weight: 0.660381\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:33.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 12600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:33.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 12600, Learning Rate: 0.008473\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:33.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 12600, Global Loss: 4.0667\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:35.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12700, Gradient Norm for user_embedding.weight: 0.961930\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:35.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12700, Gradient Norm for item_embedding.weight: 0.273296\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:35.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 12700, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:35.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 12700, Learning Rate: 0.008473\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:35.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 12700, Global Loss: 4.0815\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:37.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12800, Gradient Norm for user_embedding.weight: 0.920337\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:37.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12800, Gradient Norm for item_embedding.weight: 0.391125\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:37.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 12800, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:37.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 12800, Learning Rate: 0.008473\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:37.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 12800, Global Loss: 4.0875\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:40.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12900, Gradient Norm for user_embedding.weight: 0.920149\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:40.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 12900, Gradient Norm for item_embedding.weight: 0.391566\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:40.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 12900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:40.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 12900, Learning Rate: 0.008473\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:40.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 12900, Global Loss: 4.0903\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:42.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13000, Gradient Norm for user_embedding.weight: 0.772303\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:42.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13000, Gradient Norm for item_embedding.weight: 0.635253\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:42.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 13000, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:42.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 13000, Learning Rate: 0.007626\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:42.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 13000, Global Loss: 4.1029\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:44.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13100, Gradient Norm for user_embedding.weight: 0.967849\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:44.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13100, Gradient Norm for item_embedding.weight: 0.251531\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:44.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 13100, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:44.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 13100, Learning Rate: 0.007626\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:44.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 13100, Global Loss: 4.1226\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:46.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13200, Gradient Norm for user_embedding.weight: 0.938551\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:46.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13200, Gradient Norm for item_embedding.weight: 0.345138\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:46.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 13200, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:46.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 13200, Learning Rate: 0.007626\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:46.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 13200, Global Loss: 4.1518\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:48.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13300, Gradient Norm for user_embedding.weight: 0.920325\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:48.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13300, Gradient Norm for item_embedding.weight: 0.391152\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:48.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 13300, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:48.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 13300, Learning Rate: 0.007626\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:48.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 13300, Global Loss: 4.1765\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:50.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13400, Gradient Norm for user_embedding.weight: 0.891283\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:50.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13400, Gradient Norm for item_embedding.weight: 0.453445\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:50.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 13400, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:50.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 13400, Learning Rate: 0.007626\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:50.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 13400, Global Loss: 4.1863\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:52.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13500, Gradient Norm for user_embedding.weight: 0.994061\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:52.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13500, Gradient Norm for item_embedding.weight: 0.108819\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:52.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 13500, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:52.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 13500, Learning Rate: 0.007626\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:52.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 13500, Global Loss: 4.1894\u001b[0m\n",
      "\u001b[32m2024-09-07 23:54:54.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mEpoch 3, Loss: 4.1981\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:01.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mEpoch 3, Validation Loss: 12.4881\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/4524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 23:55:01.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13600, Gradient Norm for user_embedding.weight: 0.595938\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:01.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13600, Gradient Norm for item_embedding.weight: 0.391141\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:01.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 13600, Total Gradient Norm: 0.712834\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:01.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 13600, Learning Rate: 0.007626\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:01.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 13600, Global Loss: 2.2870\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:04.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13700, Gradient Norm for user_embedding.weight: 0.461961\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:04.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13700, Gradient Norm for item_embedding.weight: 0.255440\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:04.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 13700, Total Gradient Norm: 0.527880\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:04.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 13700, Learning Rate: 0.007626\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:04.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 13700, Global Loss: 2.1687\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:06.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13800, Gradient Norm for user_embedding.weight: 0.477441\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:06.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13800, Gradient Norm for item_embedding.weight: 0.878663\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:06.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 13800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:06.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 13800, Learning Rate: 0.007626\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:06.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 13800, Global Loss: 2.3006\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:08.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13900, Gradient Norm for user_embedding.weight: 0.629432\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:08.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 13900, Gradient Norm for item_embedding.weight: 0.418687\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:08.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 13900, Total Gradient Norm: 0.755965\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:08.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 13900, Learning Rate: 0.007626\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:08.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 13900, Global Loss: 2.3798\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:10.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14000, Gradient Norm for user_embedding.weight: 0.673782\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:10.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14000, Gradient Norm for item_embedding.weight: 0.402454\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:10.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 14000, Total Gradient Norm: 0.784825\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:10.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 14000, Learning Rate: 0.006863\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:10.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 14000, Global Loss: 2.3376\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:12.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14100, Gradient Norm for user_embedding.weight: 0.871555\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:12.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14100, Gradient Norm for item_embedding.weight: 0.418917\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:12.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 14100, Total Gradient Norm: 0.967006\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:12.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 14100, Learning Rate: 0.006863\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:12.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 14100, Global Loss: 2.3016\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:14.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14200, Gradient Norm for user_embedding.weight: 0.771545\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:14.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14200, Gradient Norm for item_embedding.weight: 0.636173\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:14.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 14200, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:14.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 14200, Learning Rate: 0.006863\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:14.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 14200, Global Loss: 2.2849\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:16.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14300, Gradient Norm for user_embedding.weight: 0.566876\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:16.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14300, Gradient Norm for item_embedding.weight: 0.823802\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:16.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 14300, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:16.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 14300, Learning Rate: 0.006863\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:16.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 14300, Global Loss: 2.2853\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:19.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14400, Gradient Norm for user_embedding.weight: 0.710250\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:19.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14400, Gradient Norm for item_embedding.weight: 0.612080\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:19.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 14400, Total Gradient Norm: 0.937602\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:19.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 14400, Learning Rate: 0.006863\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:19.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 14400, Global Loss: 2.4111\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:21.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14500, Gradient Norm for user_embedding.weight: 0.776639\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:21.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14500, Gradient Norm for item_embedding.weight: 0.452936\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:21.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 14500, Total Gradient Norm: 0.899066\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:21.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 14500, Learning Rate: 0.006863\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:21.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 14500, Global Loss: 2.4515\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:23.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14600, Gradient Norm for user_embedding.weight: 0.961346\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:23.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14600, Gradient Norm for item_embedding.weight: 0.275341\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:23.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 14600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:23.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 14600, Learning Rate: 0.006863\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:23.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 14600, Global Loss: 2.5062\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:25.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14700, Gradient Norm for user_embedding.weight: 0.384540\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:25.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14700, Gradient Norm for item_embedding.weight: 0.923107\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:25.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 14700, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:25.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 14700, Learning Rate: 0.006863\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:25.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 14700, Global Loss: 2.5525\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:27.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14800, Gradient Norm for user_embedding.weight: 0.710393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:27.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14800, Gradient Norm for item_embedding.weight: 0.325266\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:27.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 14800, Total Gradient Norm: 0.781317\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:27.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 14800, Learning Rate: 0.006863\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:27.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 14800, Global Loss: 2.5578\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:29.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14900, Gradient Norm for user_embedding.weight: 0.653688\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:29.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 14900, Gradient Norm for item_embedding.weight: 0.705152\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:29.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 14900, Total Gradient Norm: 0.961534\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:29.727\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 14900, Learning Rate: 0.006863\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:29.727\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 14900, Global Loss: 2.5790\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:31.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15000, Gradient Norm for user_embedding.weight: 0.582309\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:31.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15000, Gradient Norm for item_embedding.weight: 0.411562\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:31.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 15000, Total Gradient Norm: 0.713069\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:31.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 15000, Learning Rate: 0.006177\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:31.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 15000, Global Loss: 2.6016\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:33.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15100, Gradient Norm for user_embedding.weight: 0.692327\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:33.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15100, Gradient Norm for item_embedding.weight: 0.590651\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:33.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 15100, Total Gradient Norm: 0.910046\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:33.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 15100, Learning Rate: 0.006177\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:33.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 15100, Global Loss: 2.6087\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:36.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15200, Gradient Norm for user_embedding.weight: 0.909643\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:36.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15200, Gradient Norm for item_embedding.weight: 0.415389\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:36.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 15200, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:36.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 15200, Learning Rate: 0.006177\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:36.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 15200, Global Loss: 2.6188\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:38.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15300, Gradient Norm for user_embedding.weight: 0.799492\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:38.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15300, Gradient Norm for item_embedding.weight: 0.500970\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:38.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 15300, Total Gradient Norm: 0.943482\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:38.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 15300, Learning Rate: 0.006177\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:38.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 15300, Global Loss: 2.6179\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:40.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15400, Gradient Norm for user_embedding.weight: 0.381746\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:40.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15400, Gradient Norm for item_embedding.weight: 0.651784\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:40.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 15400, Total Gradient Norm: 0.755349\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:40.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 15400, Learning Rate: 0.006177\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:40.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 15400, Global Loss: 2.6446\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:42.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15500, Gradient Norm for user_embedding.weight: 0.651035\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:42.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15500, Gradient Norm for item_embedding.weight: 0.353659\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:42.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 15500, Total Gradient Norm: 0.740893\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:42.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 15500, Learning Rate: 0.006177\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:42.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 15500, Global Loss: 2.6738\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:44.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15600, Gradient Norm for user_embedding.weight: 0.661967\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:44.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15600, Gradient Norm for item_embedding.weight: 0.749532\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:44.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 15600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:44.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 15600, Learning Rate: 0.006177\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:44.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 15600, Global Loss: 2.6970\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:46.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15700, Gradient Norm for user_embedding.weight: 0.792417\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:46.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15700, Gradient Norm for item_embedding.weight: 0.411880\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:46.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 15700, Total Gradient Norm: 0.893067\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:46.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 15700, Learning Rate: 0.006177\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:46.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 15700, Global Loss: 2.7218\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:48.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15800, Gradient Norm for user_embedding.weight: 0.941574\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:48.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15800, Gradient Norm for item_embedding.weight: 0.336803\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:48.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 15800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:48.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 15800, Learning Rate: 0.006177\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:48.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 15800, Global Loss: 2.7609\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:50.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15900, Gradient Norm for user_embedding.weight: 0.929038\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:50.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 15900, Gradient Norm for item_embedding.weight: 0.369982\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:50.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 15900, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:50.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 15900, Learning Rate: 0.006177\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:50.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 15900, Global Loss: 2.7758\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:52.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16000, Gradient Norm for user_embedding.weight: 0.967603\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:52.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16000, Gradient Norm for item_embedding.weight: 0.252476\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:52.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 16000, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:52.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 16000, Learning Rate: 0.005559\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:52.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 16000, Global Loss: 2.7876\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:54.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16100, Gradient Norm for user_embedding.weight: 0.832791\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:54.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16100, Gradient Norm for item_embedding.weight: 0.520450\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:54.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 16100, Total Gradient Norm: 0.982044\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:54.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 16100, Learning Rate: 0.005559\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:54.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 16100, Global Loss: 2.8133\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:56.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16200, Gradient Norm for user_embedding.weight: 0.957518\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:56.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16200, Gradient Norm for item_embedding.weight: 0.288371\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:56.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 16200, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:56.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 16200, Learning Rate: 0.005559\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:56.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 16200, Global Loss: 2.8359\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:58.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16300, Gradient Norm for user_embedding.weight: 0.896176\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:58.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16300, Gradient Norm for item_embedding.weight: 0.443698\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:58.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 16300, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:58.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 16300, Learning Rate: 0.005559\u001b[0m\n",
      "\u001b[32m2024-09-07 23:55:58.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 16300, Global Loss: 2.8579\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:00.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16400, Gradient Norm for user_embedding.weight: 0.806030\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:00.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16400, Gradient Norm for item_embedding.weight: 0.591873\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:00.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 16400, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:00.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 16400, Learning Rate: 0.005559\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:00.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 16400, Global Loss: 2.8710\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:02.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16500, Gradient Norm for user_embedding.weight: 0.798890\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:02.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16500, Gradient Norm for item_embedding.weight: 0.601476\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:02.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 16500, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:02.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 16500, Learning Rate: 0.005559\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:02.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 16500, Global Loss: 2.8915\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:05.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16600, Gradient Norm for user_embedding.weight: 0.985338\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:05.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16600, Gradient Norm for item_embedding.weight: 0.170611\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:05.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 16600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:05.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 16600, Learning Rate: 0.005559\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:05.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 16600, Global Loss: 2.9106\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:07.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16700, Gradient Norm for user_embedding.weight: 0.836483\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:07.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16700, Gradient Norm for item_embedding.weight: 0.547992\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:07.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 16700, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:07.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 16700, Learning Rate: 0.005559\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:07.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 16700, Global Loss: 2.9255\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:09.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16800, Gradient Norm for user_embedding.weight: 0.423057\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:09.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16800, Gradient Norm for item_embedding.weight: 0.906102\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:09.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 16800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:09.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 16800, Learning Rate: 0.005559\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:09.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 16800, Global Loss: 2.9415\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:11.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16900, Gradient Norm for user_embedding.weight: 0.963659\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:11.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 16900, Gradient Norm for item_embedding.weight: 0.267134\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:11.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 16900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:11.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 16900, Learning Rate: 0.005559\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:11.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 16900, Global Loss: 2.9554\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:13.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17000, Gradient Norm for user_embedding.weight: 0.966839\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:13.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17000, Gradient Norm for item_embedding.weight: 0.255385\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:13.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 17000, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:13.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 17000, Learning Rate: 0.005003\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:13.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 17000, Global Loss: 2.9780\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:15.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17100, Gradient Norm for user_embedding.weight: 0.212181\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:15.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17100, Gradient Norm for item_embedding.weight: 0.106475\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:15.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 17100, Total Gradient Norm: 0.237398\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:15.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 17100, Learning Rate: 0.005003\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:15.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 17100, Global Loss: 2.9834\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:17.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17200, Gradient Norm for user_embedding.weight: 0.801410\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:17.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17200, Gradient Norm for item_embedding.weight: 0.484261\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:17.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 17200, Total Gradient Norm: 0.936359\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:17.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 17200, Learning Rate: 0.005003\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:17.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 17200, Global Loss: 3.0042\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:19.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17300, Gradient Norm for user_embedding.weight: 0.634196\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:19.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17300, Gradient Norm for item_embedding.weight: 0.342342\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:19.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 17300, Total Gradient Norm: 0.720696\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:19.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 17300, Learning Rate: 0.005003\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:19.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 17300, Global Loss: 3.0178\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:21.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17400, Gradient Norm for user_embedding.weight: 0.815594\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:21.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17400, Gradient Norm for item_embedding.weight: 0.578624\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:21.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 17400, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:21.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 17400, Learning Rate: 0.005003\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:21.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 17400, Global Loss: 3.0516\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:23.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17500, Gradient Norm for user_embedding.weight: 0.779085\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:23.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17500, Gradient Norm for item_embedding.weight: 0.376542\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:23.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 17500, Total Gradient Norm: 0.865308\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:23.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 17500, Learning Rate: 0.005003\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:23.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 17500, Global Loss: 3.0662\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:25.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17600, Gradient Norm for user_embedding.weight: 0.539963\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:25.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17600, Gradient Norm for item_embedding.weight: 0.841688\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:25.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 17600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:25.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 17600, Learning Rate: 0.005003\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:25.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 17600, Global Loss: 3.0883\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:27.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17700, Gradient Norm for user_embedding.weight: 0.955762\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:27.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17700, Gradient Norm for item_embedding.weight: 0.294137\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:27.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 17700, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:27.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 17700, Learning Rate: 0.005003\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:27.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 17700, Global Loss: 3.1016\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:29.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17800, Gradient Norm for user_embedding.weight: 0.811227\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:29.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17800, Gradient Norm for item_embedding.weight: 0.398209\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:29.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 17800, Total Gradient Norm: 0.903692\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:29.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 17800, Learning Rate: 0.005003\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:29.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 17800, Global Loss: 3.1136\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:31.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17900, Gradient Norm for user_embedding.weight: 0.651099\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:31.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 17900, Gradient Norm for item_embedding.weight: 0.614955\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:31.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 17900, Total Gradient Norm: 0.895600\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:31.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 17900, Learning Rate: 0.005003\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:31.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 17900, Global Loss: 3.1275\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:33.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18000, Gradient Norm for user_embedding.weight: 0.687091\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:33.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18000, Gradient Norm for item_embedding.weight: 0.726570\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:33.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 18000, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:33.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 18000, Learning Rate: 0.004503\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:33.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 18000, Global Loss: 3.1435\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:35.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mEpoch 4, Loss: 3.1603\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:42.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mEpoch 4, Validation Loss: 12.1557\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/4524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 23:56:42.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18100, Gradient Norm for user_embedding.weight: 0.508699\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:42.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18100, Gradient Norm for item_embedding.weight: 0.341213\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:42.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 18100, Total Gradient Norm: 0.612536\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:42.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 18100, Learning Rate: 0.004503\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:42.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 18100, Global Loss: 0.8676\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:45.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18200, Gradient Norm for user_embedding.weight: 0.939900\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:45.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18200, Gradient Norm for item_embedding.weight: 0.341447\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:45.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 18200, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:45.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 18200, Learning Rate: 0.004503\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:45.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 18200, Global Loss: 1.7241\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:46.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18300, Gradient Norm for user_embedding.weight: 0.599985\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:46.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18300, Gradient Norm for item_embedding.weight: 0.483579\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:46.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 18300, Total Gradient Norm: 0.770604\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:46.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 18300, Learning Rate: 0.004503\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:46.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 18300, Global Loss: 1.8257\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:49.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18400, Gradient Norm for user_embedding.weight: 0.436562\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:49.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18400, Gradient Norm for item_embedding.weight: 0.363192\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:49.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 18400, Total Gradient Norm: 0.567886\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:49.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 18400, Learning Rate: 0.004503\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:49.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 18400, Global Loss: 1.8352\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:51.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18500, Gradient Norm for user_embedding.weight: 0.726686\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:51.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18500, Gradient Norm for item_embedding.weight: 0.686968\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:51.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 18500, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:51.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 18500, Learning Rate: 0.004503\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:51.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 18500, Global Loss: 1.7867\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:52.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18600, Gradient Norm for user_embedding.weight: 0.747207\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:52.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18600, Gradient Norm for item_embedding.weight: 0.259391\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:52.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 18600, Total Gradient Norm: 0.790950\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:52.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 18600, Learning Rate: 0.004503\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:52.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 18600, Global Loss: 1.7993\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:54.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18700, Gradient Norm for user_embedding.weight: 0.444921\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:54.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18700, Gradient Norm for item_embedding.weight: 0.895569\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:54.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 18700, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:54.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 18700, Learning Rate: 0.004503\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:54.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 18700, Global Loss: 1.8327\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:56.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18800, Gradient Norm for user_embedding.weight: 0.562345\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:56.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18800, Gradient Norm for item_embedding.weight: 0.365257\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:56.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 18800, Total Gradient Norm: 0.670556\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:56.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 18800, Learning Rate: 0.004503\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:56.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 18800, Global Loss: 1.8660\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:58.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18900, Gradient Norm for user_embedding.weight: 0.541399\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:58.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 18900, Gradient Norm for item_embedding.weight: 0.736385\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:58.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 18900, Total Gradient Norm: 0.913989\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:58.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 18900, Learning Rate: 0.004503\u001b[0m\n",
      "\u001b[32m2024-09-07 23:56:58.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 18900, Global Loss: 1.8841\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:00.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19000, Gradient Norm for user_embedding.weight: 0.746518\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:00.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19000, Gradient Norm for item_embedding.weight: 0.386163\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:00.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 19000, Total Gradient Norm: 0.840482\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:00.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 19000, Learning Rate: 0.004053\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:00.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 19000, Global Loss: 1.8823\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:02.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19100, Gradient Norm for user_embedding.weight: 0.942359\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:02.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19100, Gradient Norm for item_embedding.weight: 0.334601\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:02.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 19100, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:02.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 19100, Learning Rate: 0.004053\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:02.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 19100, Global Loss: 1.8822\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:04.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19200, Gradient Norm for user_embedding.weight: 0.755058\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:04.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19200, Gradient Norm for item_embedding.weight: 0.655656\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:04.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 19200, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:04.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 19200, Learning Rate: 0.004053\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:04.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 19200, Global Loss: 1.9280\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:06.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19300, Gradient Norm for user_embedding.weight: 0.452237\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:06.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19300, Gradient Norm for item_embedding.weight: 0.577821\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:06.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 19300, Total Gradient Norm: 0.733755\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:06.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 19300, Learning Rate: 0.004053\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:06.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 19300, Global Loss: 1.9467\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:08.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19400, Gradient Norm for user_embedding.weight: 0.448635\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:08.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19400, Gradient Norm for item_embedding.weight: 0.296369\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:08.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 19400, Total Gradient Norm: 0.537687\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:08.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 19400, Learning Rate: 0.004053\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:08.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 19400, Global Loss: 1.9914\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:10.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19500, Gradient Norm for user_embedding.weight: 0.968743\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:10.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19500, Gradient Norm for item_embedding.weight: 0.248064\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:10.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 19500, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:10.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 19500, Learning Rate: 0.004053\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:10.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 19500, Global Loss: 2.0021\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:12.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19600, Gradient Norm for user_embedding.weight: 0.780916\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:12.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19600, Gradient Norm for item_embedding.weight: 0.309420\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:12.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 19600, Total Gradient Norm: 0.839982\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:12.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 19600, Learning Rate: 0.004053\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:12.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 19600, Global Loss: 2.0128\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:14.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19700, Gradient Norm for user_embedding.weight: 0.602247\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:14.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19700, Gradient Norm for item_embedding.weight: 0.258726\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:14.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 19700, Total Gradient Norm: 0.655470\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:14.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 19700, Learning Rate: 0.004053\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:14.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 19700, Global Loss: 2.0278\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:16.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19800, Gradient Norm for user_embedding.weight: 0.926827\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:16.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19800, Gradient Norm for item_embedding.weight: 0.375487\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:16.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 19800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:16.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 19800, Learning Rate: 0.004053\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:16.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 19800, Global Loss: 2.0640\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:18.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19900, Gradient Norm for user_embedding.weight: 0.586112\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:18.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 19900, Gradient Norm for item_embedding.weight: 0.808202\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:18.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 19900, Total Gradient Norm: 0.998358\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:18.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 19900, Learning Rate: 0.004053\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:18.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 19900, Global Loss: 2.0715\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:20.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20000, Gradient Norm for user_embedding.weight: 0.805272\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:20.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20000, Gradient Norm for item_embedding.weight: 0.592904\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:20.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 20000, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:20.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 20000, Learning Rate: 0.003647\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:20.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 20000, Global Loss: 2.1275\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:22.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20100, Gradient Norm for user_embedding.weight: 0.250750\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:22.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20100, Gradient Norm for item_embedding.weight: 0.320561\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:22.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 20100, Total Gradient Norm: 0.406983\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:22.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 20100, Learning Rate: 0.003647\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:22.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 20100, Global Loss: 2.1433\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:24.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20200, Gradient Norm for user_embedding.weight: 0.975261\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:24.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20200, Gradient Norm for item_embedding.weight: 0.221053\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:24.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 20200, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:24.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 20200, Learning Rate: 0.003647\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:24.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 20200, Global Loss: 2.1481\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:26.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20300, Gradient Norm for user_embedding.weight: 0.574039\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:26.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20300, Gradient Norm for item_embedding.weight: 0.696302\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:26.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 20300, Total Gradient Norm: 0.902418\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:26.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 20300, Learning Rate: 0.003647\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:26.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 20300, Global Loss: 2.1727\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:28.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20400, Gradient Norm for user_embedding.weight: 0.257026\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:28.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20400, Gradient Norm for item_embedding.weight: 0.313259\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:28.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 20400, Total Gradient Norm: 0.405208\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:28.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 20400, Learning Rate: 0.003647\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:28.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 20400, Global Loss: 2.1817\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:30.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20500, Gradient Norm for user_embedding.weight: 0.738002\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:30.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20500, Gradient Norm for item_embedding.weight: 0.260091\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:30.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 20500, Total Gradient Norm: 0.782492\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:30.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 20500, Learning Rate: 0.003647\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:30.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 20500, Global Loss: 2.2035\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:32.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20600, Gradient Norm for user_embedding.weight: 0.955037\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:32.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20600, Gradient Norm for item_embedding.weight: 0.296485\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:32.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 20600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:32.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 20600, Learning Rate: 0.003647\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:32.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 20600, Global Loss: 2.2167\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:34.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20700, Gradient Norm for user_embedding.weight: 0.327692\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:34.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20700, Gradient Norm for item_embedding.weight: 0.305661\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:34.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 20700, Total Gradient Norm: 0.448119\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:34.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 20700, Learning Rate: 0.003647\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:34.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 20700, Global Loss: 2.2255\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:36.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20800, Gradient Norm for user_embedding.weight: 0.897913\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:36.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20800, Gradient Norm for item_embedding.weight: 0.440172\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:36.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 20800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:36.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 20800, Learning Rate: 0.003647\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:36.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 20800, Global Loss: 2.2306\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:38.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20900, Gradient Norm for user_embedding.weight: 0.422124\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:38.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 20900, Gradient Norm for item_embedding.weight: 0.664268\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:38.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 20900, Total Gradient Norm: 0.787046\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:38.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 20900, Learning Rate: 0.003647\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:38.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 20900, Global Loss: 2.2770\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:40.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21000, Gradient Norm for user_embedding.weight: 0.410112\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:40.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21000, Gradient Norm for item_embedding.weight: 0.493704\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:40.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 21000, Total Gradient Norm: 0.641822\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:40.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 21000, Learning Rate: 0.003283\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:40.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 21000, Global Loss: 2.2906\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:42.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21100, Gradient Norm for user_embedding.weight: 0.661564\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:42.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21100, Gradient Norm for item_embedding.weight: 0.421410\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:42.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 21100, Total Gradient Norm: 0.784380\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:42.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 21100, Learning Rate: 0.003283\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:42.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 21100, Global Loss: 2.2962\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:44.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21200, Gradient Norm for user_embedding.weight: 0.664515\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:44.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21200, Gradient Norm for item_embedding.weight: 0.554546\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:44.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 21200, Total Gradient Norm: 0.865507\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:44.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 21200, Learning Rate: 0.003283\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:44.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 21200, Global Loss: 2.3077\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:46.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21300, Gradient Norm for user_embedding.weight: 0.394929\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:46.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21300, Gradient Norm for item_embedding.weight: 0.293636\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:46.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 21300, Total Gradient Norm: 0.492129\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:46.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 21300, Learning Rate: 0.003283\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:46.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 21300, Global Loss: 2.3250\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:48.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21400, Gradient Norm for user_embedding.weight: 0.133388\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:48.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21400, Gradient Norm for item_embedding.weight: 0.139351\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:48.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 21400, Total Gradient Norm: 0.192902\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:48.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 21400, Learning Rate: 0.003283\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:48.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 21400, Global Loss: 2.3435\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:50.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21500, Gradient Norm for user_embedding.weight: 0.974074\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:50.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21500, Gradient Norm for item_embedding.weight: 0.226226\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:50.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 21500, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:50.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 21500, Learning Rate: 0.003283\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:50.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 21500, Global Loss: 2.3694\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:52.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21600, Gradient Norm for user_embedding.weight: 0.972395\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:52.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21600, Gradient Norm for item_embedding.weight: 0.233337\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:52.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 21600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:52.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 21600, Learning Rate: 0.003283\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:52.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 21600, Global Loss: 2.3870\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:54.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21700, Gradient Norm for user_embedding.weight: 0.922864\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:54.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21700, Gradient Norm for item_embedding.weight: 0.385126\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:54.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 21700, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:54.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 21700, Learning Rate: 0.003283\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:54.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 21700, Global Loss: 2.3971\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:56.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21800, Gradient Norm for user_embedding.weight: 0.766848\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:56.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21800, Gradient Norm for item_embedding.weight: 0.583122\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:56.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 21800, Total Gradient Norm: 0.963373\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:56.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 21800, Learning Rate: 0.003283\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:56.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 21800, Global Loss: 2.4165\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:58.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21900, Gradient Norm for user_embedding.weight: 0.400601\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:58.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 21900, Gradient Norm for item_embedding.weight: 0.916252\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:58.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 21900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:58.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 21900, Learning Rate: 0.003283\u001b[0m\n",
      "\u001b[32m2024-09-07 23:57:58.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 21900, Global Loss: 2.4492\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:00.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22000, Gradient Norm for user_embedding.weight: 0.819685\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:00.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22000, Gradient Norm for item_embedding.weight: 0.572813\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:00.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 22000, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:00.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 22000, Learning Rate: 0.002954\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:00.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 22000, Global Loss: 2.4634\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:02.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22100, Gradient Norm for user_embedding.weight: 0.729124\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:02.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22100, Gradient Norm for item_embedding.weight: 0.354604\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:02.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 22100, Total Gradient Norm: 0.810781\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:02.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 22100, Learning Rate: 0.002954\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:02.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 22100, Global Loss: 2.4888\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:04.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22200, Gradient Norm for user_embedding.weight: 0.711933\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:04.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22200, Gradient Norm for item_embedding.weight: 0.702247\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:04.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 22200, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:04.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 22200, Learning Rate: 0.002954\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:04.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 22200, Global Loss: 2.5086\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:06.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22300, Gradient Norm for user_embedding.weight: 0.412445\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:06.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22300, Gradient Norm for item_embedding.weight: 0.204781\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:06.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 22300, Total Gradient Norm: 0.460484\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:06.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 22300, Learning Rate: 0.002954\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:06.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 22300, Global Loss: 2.5247\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:08.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22400, Gradient Norm for user_embedding.weight: 0.862933\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:08.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22400, Gradient Norm for item_embedding.weight: 0.505317\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:08.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 22400, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:08.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 22400, Learning Rate: 0.002954\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:08.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 22400, Global Loss: 2.5385\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:10.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22500, Gradient Norm for user_embedding.weight: 0.997994\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:10.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22500, Gradient Norm for item_embedding.weight: 0.063298\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:10.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 22500, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:10.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 22500, Learning Rate: 0.002954\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:10.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 22500, Global Loss: 2.5598\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:12.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22600, Gradient Norm for user_embedding.weight: 0.937307\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:12.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22600, Gradient Norm for item_embedding.weight: 0.348503\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:12.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 22600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:12.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 22600, Learning Rate: 0.002954\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:12.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 22600, Global Loss: 2.5725\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:13.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mEpoch 5, Loss: 2.5777\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:20.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mEpoch 5, Validation Loss: 12.1523\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/4524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 23:58:22.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22700, Gradient Norm for user_embedding.weight: 0.933609\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:22.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22700, Gradient Norm for item_embedding.weight: 0.293201\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:22.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 22700, Total Gradient Norm: 0.978567\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:22.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 22700, Learning Rate: 0.002954\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:22.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 22700, Global Loss: 1.9047\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:24.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22800, Gradient Norm for user_embedding.weight: 0.678215\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:24.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22800, Gradient Norm for item_embedding.weight: 0.223477\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:24.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 22800, Total Gradient Norm: 0.714085\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:24.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 22800, Learning Rate: 0.002954\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:24.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 22800, Global Loss: 1.8526\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:26.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22900, Gradient Norm for user_embedding.weight: 0.984458\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:26.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 22900, Gradient Norm for item_embedding.weight: 0.175619\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:26.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 22900, Total Gradient Norm: 1.000000\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:26.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 22900, Learning Rate: 0.002954\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:26.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 22900, Global Loss: 1.7555\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:29.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23000, Gradient Norm for user_embedding.weight: 0.464563\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:29.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23000, Gradient Norm for item_embedding.weight: 0.214377\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:29.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 23000, Total Gradient Norm: 0.511641\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:29.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 23000, Learning Rate: 0.002659\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:29.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 23000, Global Loss: 1.7518\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:31.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23100, Gradient Norm for user_embedding.weight: 0.874221\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:31.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23100, Gradient Norm for item_embedding.weight: 0.485526\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:31.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 23100, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:31.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 23100, Learning Rate: 0.002659\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:31.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 23100, Global Loss: 1.7509\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:33.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23200, Gradient Norm for user_embedding.weight: 0.616039\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:33.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23200, Gradient Norm for item_embedding.weight: 0.575509\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:33.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 23200, Total Gradient Norm: 0.843039\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:33.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 23200, Learning Rate: 0.002659\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:33.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 23200, Global Loss: 1.7686\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:35.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23300, Gradient Norm for user_embedding.weight: 0.581417\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:35.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23300, Gradient Norm for item_embedding.weight: 0.813605\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:35.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 23300, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:35.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 23300, Learning Rate: 0.002659\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:35.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 23300, Global Loss: 1.7461\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:37.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23400, Gradient Norm for user_embedding.weight: 0.513336\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:37.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23400, Gradient Norm for item_embedding.weight: 0.491484\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:37.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 23400, Total Gradient Norm: 0.710684\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:37.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 23400, Learning Rate: 0.002659\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:37.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 23400, Global Loss: 1.7543\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:39.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23500, Gradient Norm for user_embedding.weight: 0.968555\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:39.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23500, Gradient Norm for item_embedding.weight: 0.248797\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:39.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 23500, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:39.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 23500, Learning Rate: 0.002659\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:39.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 23500, Global Loss: 1.8175\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:41.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23600, Gradient Norm for user_embedding.weight: 0.549285\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:41.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23600, Gradient Norm for item_embedding.weight: 0.272302\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:41.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 23600, Total Gradient Norm: 0.613077\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:41.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 23600, Learning Rate: 0.002659\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:41.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 23600, Global Loss: 1.8132\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:43.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23700, Gradient Norm for user_embedding.weight: 0.374480\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:43.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23700, Gradient Norm for item_embedding.weight: 0.812196\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:43.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 23700, Total Gradient Norm: 0.894370\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:43.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 23700, Learning Rate: 0.002659\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:43.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 23700, Global Loss: 1.8297\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:45.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23800, Gradient Norm for user_embedding.weight: 0.946626\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:45.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23800, Gradient Norm for item_embedding.weight: 0.322331\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:45.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 23800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:45.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 23800, Learning Rate: 0.002659\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:45.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 23800, Global Loss: 1.8364\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:47.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23900, Gradient Norm for user_embedding.weight: 0.616257\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:47.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 23900, Gradient Norm for item_embedding.weight: 0.399118\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:47.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 23900, Total Gradient Norm: 0.734212\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:47.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 23900, Learning Rate: 0.002659\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:47.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 23900, Global Loss: 1.8256\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:49.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24000, Gradient Norm for user_embedding.weight: 0.544318\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:49.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24000, Gradient Norm for item_embedding.weight: 0.313241\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:49.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 24000, Total Gradient Norm: 0.628014\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:49.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 24000, Learning Rate: 0.002393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:49.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 24000, Global Loss: 1.8199\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:51.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24100, Gradient Norm for user_embedding.weight: 0.971621\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:51.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24100, Gradient Norm for item_embedding.weight: 0.236540\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:51.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 24100, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:51.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 24100, Learning Rate: 0.002393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:51.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 24100, Global Loss: 1.8251\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:53.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24200, Gradient Norm for user_embedding.weight: 0.859646\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:53.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24200, Gradient Norm for item_embedding.weight: 0.362860\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:53.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 24200, Total Gradient Norm: 0.933091\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:53.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 24200, Learning Rate: 0.002393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:53.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 24200, Global Loss: 1.8222\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:55.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24300, Gradient Norm for user_embedding.weight: 0.783790\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:55.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24300, Gradient Norm for item_embedding.weight: 0.418875\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:55.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 24300, Total Gradient Norm: 0.888697\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:55.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 24300, Learning Rate: 0.002393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:55.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 24300, Global Loss: 1.8302\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:57.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24400, Gradient Norm for user_embedding.weight: 0.522510\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:57.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24400, Gradient Norm for item_embedding.weight: 0.431146\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:57.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 24400, Total Gradient Norm: 0.677424\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:57.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 24400, Learning Rate: 0.002393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:57.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 24400, Global Loss: 1.8435\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:59.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24500, Gradient Norm for user_embedding.weight: 0.959495\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:59.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24500, Gradient Norm for item_embedding.weight: 0.281722\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:59.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 24500, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:59.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 24500, Learning Rate: 0.002393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:58:59.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 24500, Global Loss: 1.8544\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:01.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24600, Gradient Norm for user_embedding.weight: 0.687441\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:01.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24600, Gradient Norm for item_embedding.weight: 0.245210\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:01.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 24600, Total Gradient Norm: 0.729865\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:01.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 24600, Learning Rate: 0.002393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:01.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 24600, Global Loss: 1.8667\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:03.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24700, Gradient Norm for user_embedding.weight: 0.351895\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:04.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24700, Gradient Norm for item_embedding.weight: 0.254937\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:04.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 24700, Total Gradient Norm: 0.434537\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:04.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 24700, Learning Rate: 0.002393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:04.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 24700, Global Loss: 1.8714\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:06.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24800, Gradient Norm for user_embedding.weight: 0.801591\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:06.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24800, Gradient Norm for item_embedding.weight: 0.441201\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:06.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 24800, Total Gradient Norm: 0.914990\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:06.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 24800, Learning Rate: 0.002393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:06.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 24800, Global Loss: 1.8818\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:08.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24900, Gradient Norm for user_embedding.weight: 0.982225\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:08.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 24900, Gradient Norm for item_embedding.weight: 0.187704\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:08.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 24900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:08.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 24900, Learning Rate: 0.002393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:08.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 24900, Global Loss: 1.8847\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:10.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25000, Gradient Norm for user_embedding.weight: 0.920238\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:10.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25000, Gradient Norm for item_embedding.weight: 0.391358\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:10.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 25000, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:10.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 25000, Learning Rate: 0.002154\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:10.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 25000, Global Loss: 1.9150\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:12.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25100, Gradient Norm for user_embedding.weight: 0.545867\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:12.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25100, Gradient Norm for item_embedding.weight: 0.424774\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:12.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 25100, Total Gradient Norm: 0.691667\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:12.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 25100, Learning Rate: 0.002154\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:12.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 25100, Global Loss: 1.9339\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:14.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25200, Gradient Norm for user_embedding.weight: 0.629185\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:14.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25200, Gradient Norm for item_embedding.weight: 0.194868\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:14.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 25200, Total Gradient Norm: 0.658671\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:14.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 25200, Learning Rate: 0.002154\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:14.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 25200, Global Loss: 1.9553\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:16.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25300, Gradient Norm for user_embedding.weight: 0.501964\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:16.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25300, Gradient Norm for item_embedding.weight: 0.788747\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:16.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 25300, Total Gradient Norm: 0.934928\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:16.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 25300, Learning Rate: 0.002154\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:16.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 25300, Global Loss: 1.9541\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:18.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25400, Gradient Norm for user_embedding.weight: 0.574346\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:18.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25400, Gradient Norm for item_embedding.weight: 0.670069\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:18.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 25400, Total Gradient Norm: 0.882534\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:18.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 25400, Learning Rate: 0.002154\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:18.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 25400, Global Loss: 1.9752\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:20.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25500, Gradient Norm for user_embedding.weight: 0.985614\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:20.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25500, Gradient Norm for item_embedding.weight: 0.169006\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:20.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 25500, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:20.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 25500, Learning Rate: 0.002154\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:20.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 25500, Global Loss: 1.9890\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:22.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25600, Gradient Norm for user_embedding.weight: 0.513941\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:22.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25600, Gradient Norm for item_embedding.weight: 0.522586\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:22.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 25600, Total Gradient Norm: 0.732960\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:22.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 25600, Learning Rate: 0.002154\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:22.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 25600, Global Loss: 2.0078\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:24.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25700, Gradient Norm for user_embedding.weight: 0.978958\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:24.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25700, Gradient Norm for item_embedding.weight: 0.204060\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:24.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 25700, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:24.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 25700, Learning Rate: 0.002154\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:24.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 25700, Global Loss: 2.0101\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:26.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25800, Gradient Norm for user_embedding.weight: 0.286086\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:26.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25800, Gradient Norm for item_embedding.weight: 0.162357\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:26.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 25800, Total Gradient Norm: 0.328945\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:26.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 25800, Learning Rate: 0.002154\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:26.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 25800, Global Loss: 2.0259\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:28.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25900, Gradient Norm for user_embedding.weight: 0.930633\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:28.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 25900, Gradient Norm for item_embedding.weight: 0.365952\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:28.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 25900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:28.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 25900, Learning Rate: 0.002154\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:28.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 25900, Global Loss: 2.0353\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:30.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26000, Gradient Norm for user_embedding.weight: 0.867178\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:30.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26000, Gradient Norm for item_embedding.weight: 0.497997\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:30.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 26000, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:30.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 26000, Learning Rate: 0.001938\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:30.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 26000, Global Loss: 2.0460\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:32.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26100, Gradient Norm for user_embedding.weight: 0.918782\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:32.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26100, Gradient Norm for item_embedding.weight: 0.394763\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:32.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 26100, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:32.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 26100, Learning Rate: 0.001938\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:32.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 26100, Global Loss: 2.0711\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:34.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26200, Gradient Norm for user_embedding.weight: 0.528235\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:34.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26200, Gradient Norm for item_embedding.weight: 0.318793\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:34.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 26200, Total Gradient Norm: 0.616977\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:34.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 26200, Learning Rate: 0.001938\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:34.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 26200, Global Loss: 2.0920\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:36.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26300, Gradient Norm for user_embedding.weight: 0.891171\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:36.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26300, Gradient Norm for item_embedding.weight: 0.453665\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:36.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 26300, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:36.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 26300, Learning Rate: 0.001938\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:36.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 26300, Global Loss: 2.0951\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:38.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26400, Gradient Norm for user_embedding.weight: 0.262042\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:38.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26400, Gradient Norm for item_embedding.weight: 0.290201\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:38.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 26400, Total Gradient Norm: 0.391002\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:38.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 26400, Learning Rate: 0.001938\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:38.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 26400, Global Loss: 2.1079\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:40.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26500, Gradient Norm for user_embedding.weight: 0.815260\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:40.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26500, Gradient Norm for item_embedding.weight: 0.579094\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:40.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 26500, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:40.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 26500, Learning Rate: 0.001938\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:40.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 26500, Global Loss: 2.1163\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:42.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26600, Gradient Norm for user_embedding.weight: 0.965881\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:42.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26600, Gradient Norm for item_embedding.weight: 0.258983\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:42.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 26600, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:42.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 26600, Learning Rate: 0.001938\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:42.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 26600, Global Loss: 2.1286\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:45.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26700, Gradient Norm for user_embedding.weight: 0.796192\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:45.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26700, Gradient Norm for item_embedding.weight: 0.356843\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:45.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 26700, Total Gradient Norm: 0.872501\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:45.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 26700, Learning Rate: 0.001938\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:45.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 26700, Global Loss: 2.1362\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:47.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26800, Gradient Norm for user_embedding.weight: 0.892602\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:47.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26800, Gradient Norm for item_embedding.weight: 0.450843\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:47.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 26800, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:47.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 26800, Learning Rate: 0.001938\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:47.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 26800, Global Loss: 2.1561\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:49.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26900, Gradient Norm for user_embedding.weight: 0.923871\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:49.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 26900, Gradient Norm for item_embedding.weight: 0.382701\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:49.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 26900, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:49.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 26900, Learning Rate: 0.001938\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:49.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 26900, Global Loss: 2.1661\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:51.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 27000, Gradient Norm for user_embedding.weight: 0.342111\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:51.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 27000, Gradient Norm for item_embedding.weight: 0.248393\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:51.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 27000, Total Gradient Norm: 0.422775\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:51.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 27000, Learning Rate: 0.001744\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:51.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 27000, Global Loss: 2.1839\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:53.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 27100, Gradient Norm for user_embedding.weight: 0.976158\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:53.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mStep 27100, Gradient Norm for item_embedding.weight: 0.217058\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:53.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mlog_gradients\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mStep 27100, Total Gradient Norm: 0.999999\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:53.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mStep 27100, Learning Rate: 0.001744\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:53.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStep 27100, Global Loss: 2.1920\u001b[0m\n",
      "\u001b[32m2024-09-07 23:59:54.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mEpoch 6, Loss: 2.1997\u001b[0m\n",
      "\u001b[32m2024-09-08 00:00:01.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mEpoch 6, Validation Loss: 12.1274\u001b[0m\n",
      "\u001b[32m2024-09-08 00:00:01.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train_utils\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mEarly stopping at epoch 6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "train(model, dataloader, val_dataloader, epochs=n_epochs, patience=2, print_steps=100, lr=0.03, device=device, progress_bar_type='tqdm_notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07c8e4-0e63-4e60-b865-4ee6b93c6524",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ebe0aa1-0001-47f5-8488-ae784d0e5558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEVPPTMG43C6GWSR7I2UGRQN7WFQ</td>\n",
       "      <td>B0863MT183</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1613701986538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AEVPPTMG43C6GWSR7I2UGRQN7WFQ</td>\n",
       "      <td>B08P8P7686</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1613702112995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEVPPTMG43C6GWSR7I2UGRQN7WFQ</td>\n",
       "      <td>B0B7LV3DN2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1617641445475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEVPPTMG43C6GWSR7I2UGRQN7WFQ</td>\n",
       "      <td>B09WMQ6DXG</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1620231368468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>AHV6QCNBJNSGLATP56JAWJ3C4G2A</td>\n",
       "      <td>B019WRM1IA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1451860309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735704</th>\n",
       "      <td>AHS2PQ33BWQLXC5NNUZS2BFXD34Q</td>\n",
       "      <td>B07TZT67KX</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1622844181866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735800</th>\n",
       "      <td>AFO5SNKILFVJMSJJ2E3BRLDGE4NA</td>\n",
       "      <td>B09T5VN7D1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1601154352542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735801</th>\n",
       "      <td>AFO5SNKILFVJMSJJ2E3BRLDGE4NA</td>\n",
       "      <td>B09918MSTF</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1602615880364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736772</th>\n",
       "      <td>AEFPHMM7CLX4UJNXJFQF4ZF5GNAA</td>\n",
       "      <td>B07P27XFP7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1599585146628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736773</th>\n",
       "      <td>AEFPHMM7CLX4UJNXJFQF4ZF5GNAA</td>\n",
       "      <td>B09ZPHLDSF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1619848589277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18095 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             user_id parent_asin  rating      timestamp\n",
       "1       AEVPPTMG43C6GWSR7I2UGRQN7WFQ  B0863MT183     4.0  1613701986538\n",
       "2       AEVPPTMG43C6GWSR7I2UGRQN7WFQ  B08P8P7686     5.0  1613702112995\n",
       "3       AEVPPTMG43C6GWSR7I2UGRQN7WFQ  B0B7LV3DN2     4.0  1617641445475\n",
       "4       AEVPPTMG43C6GWSR7I2UGRQN7WFQ  B09WMQ6DXG     5.0  1620231368468\n",
       "70      AHV6QCNBJNSGLATP56JAWJ3C4G2A  B019WRM1IA     5.0  1451860309000\n",
       "...                              ...         ...     ...            ...\n",
       "735704  AHS2PQ33BWQLXC5NNUZS2BFXD34Q  B07TZT67KX     5.0  1622844181866\n",
       "735800  AFO5SNKILFVJMSJJ2E3BRLDGE4NA  B09T5VN7D1     4.0  1601154352542\n",
       "735801  AFO5SNKILFVJMSJJ2E3BRLDGE4NA  B09918MSTF     5.0  1602615880364\n",
       "736772  AEFPHMM7CLX4UJNXJFQF4ZF5GNAA  B07P27XFP7     5.0  1599585146628\n",
       "736773  AEFPHMM7CLX4UJNXJFQF4ZF5GNAA  B09ZPHLDSF     1.0  1619848589277\n",
       "\n",
       "[18095 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "568e6612-f7e5-4960-8903-5a87edb42d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34367</th>\n",
       "      <td>AEHW2B54HDLZ3APBEWXHYLZ6SSYQ</td>\n",
       "      <td>B07MYVF61Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1654225907045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id parent_asin  rating      timestamp\n",
       "34367  AEHW2B54HDLZ3APBEWXHYLZ6SSYQ  B07MYVF61Y     4.0  1654225907045"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = 'AEHW2B54HDLZ3APBEWXHYLZ6SSYQ'\n",
    "val_df.loc[lambda df: df['user_id'].eq(user_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aad5b21-6234-41a5-b922-c7bd77ba7011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.6992], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id = 'B07MYVF61Y'\n",
    "user_indice = idm.get_user_index(user_id)\n",
    "item_indice = idm.get_item_index(item_id)\n",
    "\n",
    "model.predict([user_indice], [item_indice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623cdc93-abd9-4fa9-9553-a2c37da64e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce6982-efec-4a48-afe9-8bbdbcbe1aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
